[{"title":"《存在主义心理治疗》总结","path":"/2025/12/04/《存在主义心理治疗》总结/","content":"以存在主义为导向的心理治疗教科书，探讨了死亡、自由、孤独和无意义对心理病理构成的影响，以及对应的心理治疗意见。倡导人需接受自己必死的命运、认识到自己在这个世界上终究孤独一人，必须行使自由构建自己的世界与意义。 1. 总述 存在主义是20世纪兴起的哲学思潮，强调个体存在的独特性和优先性，认为“存在先于本质”——人并非生来就被预设了固定的意义或目的，而是通过自己的选择、行动和体验，在自由与责任中不断塑造自我，创造属于自己的价值与意义。个人理解就是一种积极的虚无主义，在大前提上接受了世界的无意义，但强调正是因为没有绝对的价值，那么人就相应地有了绝对的自由，可以积极地去构建自己的世界。 作者对于存在主义心理治疗的定义：存在主义心理治疗是一种动力性治疗方法，其焦点在于根植于个体存在中的关怀（concerns）。 所谓动力性来自于弗洛伊德关于心理功能的动力性模型：在个体内部具有冲突性力量，无论是适应性的还是心理病理性的思想、情感和行为，都是这些冲突的产物，而且这些力量存在于意识到不同层面。存在主义心理治疗与传统心理治疗流派的不同在于对冲突性力量来源问题的不同判定。 定义中的“关怀”就是存在主义心理治疗所认定的冲突性力量来源。如果我们从日常世界中抽离，并对自身的存在进行深刻地反思，最终会认识到关于存在的一些既定事实，这些事实被称为终极关怀（ultimate concerns，个人认为翻译为终极虑题之类的更恰当更无歧义），个体与这些终极关怀的交锋形成了心理上的冲突性力量。而作者在本书中主要探讨四种终极关怀：死亡、自由、孤独和无意义。 这四种终极关怀构成存在主义心理治疗的主体，组成其基本动力性结构：对终极关怀的觉察→焦虑→防御机制。人类的所有个体都处于存在主义困境当中，但某些个体无力防御或者防御不当，最终造成了心理病理的出现。 2. 死亡 对于死亡，作者提出以下两个基本命题： 生命与死亡相依存，死亡在生命表层之下持续骚动，并对经验和行为产生巨大影响。 死亡是焦虑的原始来源，因此也是心理病理的根本源头。 死亡对于任何一个个体来说都是最不证自明的道理，但对死亡的焦虑却深藏在人们建立的大量心理防御机制之下。马丁·海德格尔认为人通常活在“忘失”的生活状态中，倾向于逃避其对自身生命所负的全部责任，而直面死亡以及和死亡相关的紧急体验能够让人脱离忘失，进入到“念兹在兹”的状态，从而保持对自身存在的注意并承担起生命的责任。因此说肉体的死亡会毁灭人，而死亡的观念却能拯救人。 人清醒地意识到自己终将化为虚无，对这种认识的恐惧如此巨大，以至于人将生命中大部分能量都用于潜藏死亡恐惧或者对死亡进行否认，这些否认机制导致了死亡焦虑通常不会以赤裸裸的形态表现在临床治疗上，而是以各种其他形态出现。而涉世未深的儿童尚未形成对于死亡焦虑的完整防御机制，因此非常适合作为研究人类如何与死亡进行搏斗的对象。通过列举大量有关儿童死亡观念的研究，作者总结了人类否认死亡的两种基本形式： 独特性。人类从儿童时期到成人时期，一直存在认为自己有独特性的非理性观念。人固有一死，但我是例外。 终极拯救者。渴望有一个全能的保护者（权威、神、伴侣、意识形态）能将自己从命运的偶然和死亡中拯救出来。 具有独特性信念的人在潜意识里相信普遍的法则不适用于我，即使我们每个人都在理性层面上清楚地知道我们与他人没有任何不同。世界不会承认任何个体的特殊性，当死亡真的来到他们身边的时候，他们通常会选择进行某种形式上的否定来缓解潜在的认知失调，不肯面对疾病治疗、不愿相信自己死后世界依然照常运转、认为遭到了生命的背叛而实际上生命从来没有许诺过任何事情。而在日常生活中，他们中的一些会极度坚持个体化，拼命追求财富、名声、权力或独特技艺，潜意识中希望这些非凡的特质能让自己成为“被命运选中的人”，追求一种象征性的不朽，从而豁免于普通人的结局；另一些则由于过度重视自身独特性而忽视了他人的权益，最终形成自恋型人格；还有一些为了减轻自己的恐惧和限制感，选择膨胀自己的控制欲，通过扩大自身能控制的事情来逃避焦虑。即便他们已经取得了无以复加的成功，也仍然摆脱不了内心的焦虑与不安，出类拔萃、鹤立鸡群，人要成为他自己的神，极端的独特就是极端的孤独，意味着无所依靠，没有相依相偎的安慰，很少有个体能够忍受如此无所遮挡的个体化孤独。当独特性的神话崩塌，人就会可能转向另一个否认机制：寻求一个终极拯救者。 具有终极拯救者信念的人相信存在一个全能的、高于我的力量，只要我与之联结、服从或取悦它，就能获得庇护，免于消亡与无助。这种信念贯穿人类整个历史，没有哪个早期文化认为人类是孤独地存在于这个冷漠世界之上的。人类以这种信念克服对死亡的恐惧，出于对全能的力量、崇高的地位或是人格化目标的热忱，而选择放弃自由，在自己创造出来的安全错觉中尽情享受。行使完全自由的权利就代表着要承担完全的责任，如果我们做出了错误的选择，那么只能将其归因于自己，拒绝承担生命的责任的人将终极拯救者当作人生的指南针，放弃选择的自由而将拯救者作为选择的标准，从而变相地规避了责任。然而这种信念比起独特性信念更容易破灭，也更限制个体的人生，让个体倾向于避免生命中的冒险与挑战。 大部分人会同时使用这两种信念来构建自己的防御机制。因为自己是独特的，所以全能的力量会拯救自己；或是因为有全能的力量在保护自己，所以自己是独特的。人在这两种信念之间摇摆，一方面渴望自由并增长自己的生命，另一方面又害怕自由所带来的孤独和缺乏保护，渴望融入一个更大的整体。 撕破两种基本防御机制，让个体觉察死亡可以使生命观发生巨大的改变。死亡作为一种边界处境，迫使个体面对自己存在于世的处境，并推动个体脱离对琐事的关心，实现一种更高的存在状态。死亡觉察通过以下机制来起作用： 认识到生命无法拖延。个体不再把生活推迟到未来某个时刻，认识到人能真正生活的只有当下，如果他将在今天死去，那么对于明天的一切打算和诺言都将终结。 盘点人生的福气。个体开始在意自己真正拥有的和能做的事，对数不尽的存在的馈赠保持欣赏和感恩之心。 去认同（disidentification）。将存在与认同的事物区分开来，即存在不等于拥有的事物（工作、名声、角色），所有除存在以外的东西都不是你，它们都会消失，但你仍将存在。 以前听说过一句话，“幸福感来自于增量而非存量”，死亡将人的生命体验直接拉到谷底，原本生活中习以为常的事物对于在死亡边缘摇摇欲坠的生命来说变成了巨大的增量，从而激发人对生命的感激。 3. 自由 人要掌握自由就必须负担起相应的责任， 个体自由地构建了自己和自己的世界，因此需要对自己的生活负责的只有自己，如果不是个人的创造，那么世上万物都没有意义，宇宙中不存在规则、道德系统、价值、任何外在的参照物和宏大的计划。当人们觉察到自己如此重大的责任时，常常会产生一种无根感，过去所坚信的信念体系就像地面突然消失一样崩塌，让个体感觉无所立足、无所依靠。 但我们对自由仍然准备不足，太多的东西要去承受，太多焦虑需要释放，人们找寻各种方法来把自己与自由隔离开。对于那些深入思考便会让自己意识到根本性的无根感的情景，例如做决定、孤独、自主的活动，个体唯恐避之不及，因此人们追求秩序、权威、宏大的图景等，即使是暴君，也比没有领袖来得好，个体会因为自由而感到不安，会要求设定限制。个体常见的逃避自由与责任的方式有以下几种： 强迫性：认定世界有某种不可抗拒的力量在影响自己的行为，从而建构一个没有自由体验的精神世界。 责任的转换：将责任转换到他人身上，相信自己的处境和不快是外力造成的。 否认责任：把自己看作是无端被卷入事件的受害者。 拒绝责任：进入一种非理性的情绪失控状态，做出各种不负责任的行为。 常用于拒绝责任的一个理由是“有很多事情是无法改变的”：人必须谋生、对自己的孩子来说必须是父母、必须实现自身的道德义务、穷人没有退休的自由，诸如此类，这是对人类自由改变的根本性反驳，是受到环境决定论影响而产生的信念。绝对的环境决定论是过于极端的观点，如果我们是由环境控制的，那么又是谁在操控环境。为了使自由意志和环境影响相容，交互决定论被提出，其认为行为和环境有一种交互关系，行为能够影响环境，环境不是既定事实，而和行为一样有原因。行为主义者对此提出的质疑是无论人的认知差异有多大，当他们落入水中时的行为都会非常相似，溺水这一环境直接导向了这一行为。作者则认为不应该以行为来作为测量自由的标准，人类的生理反应是有限的，但人可以选择对处境的感受、态度，心理反应的范围是没有止境的。环境对人的限制作用是无法否认的，但这不代表我们在这种处境中就毫无责任，我们对面对困境的态度有责任（所谓逆商），对是否会被痛苦、愤怒、沮丧所击倒负有责任，即使所有努力都失败，逆境无法克服，人也要为自己对逆境采取的态度负责，是懊恼地生活，还是超越逆境重建有意义的生活。 作者/存在主义心理治疗在这一章里把个体责任抬到了一个相当的高度，尽管后续介绍了决定论、自由意志论和交互决定论，但是整体上还是偏向于认为个体创造了自己的大部分困难，格局显得有些小气。在我看来这种责任疗法更多是发现自由而非学会自由，比如中产阶级可以因为不满意的人际关系直接辞职，而对穷人来说辞职可能意味着无收入的生存困境，这种自由的成本根本负担不起。每个人都有自由选择的权利，但是每个人可以自由选择的选项是不一样多的。拥有更多外部自由的中产阶级可以承担每小时三位数的心理咨询费用，治疗师只需要让他们发现自己拥有多大的自由就足以带来行为上的改变，而这种正向的疗效又进一步强化治疗师和被治疗者对所谓“自由选择”的信念，并拿来指责那些没有多少外部自由的弱者，“你之所以有这么多困境，是因为你不敢选择”。 当病人对责任的觉察开始增强之后，通常会出现一种存在性内疚感，因为存在主义中为自己的行为负完全责任这个观点减少了逃避内疚的渠道，也扩展了内疚的范围，个体不能再依赖于一些常见的借口，他会为自己过去犯下的错误感到内疚，这种内疚的核心点在于对自己的违抗，自己没有尽己所能地去生活，没有发挥出自己的能力和潜力。存在性内疚具有积极的建设性作用，通过这种内疚人能认出自己的潜力，知道自己已经迷失，从而引导个体回归自我。 对责任的觉察只是治疗的第一步，为了激发病人的行动，需要先激发其改变的意志与愿望。对于没有愿望的病人，即便你给了他自由，他只会疑惑自己到底该做什么，因此一个人要先了解自己的渴望才能为自己而行动。比较常见的愿望受阻的个体通常是怀疑或者抑制自己的愿望，因为未实现的愿望容易使人受伤、或者是希望永恒的照顾者能够读出自己愿望、再或者是害怕因为表达自己的愿望而被照顾者抛弃。通常疗法的共同核心都是促进人际关系从而促进人的愿望的能力。 一旦个体充分体验到愿望，就要开始面对决定和选择，这一节点受阻是因为做决定通常触及到个体的存在性根源。当个体做出决定时，总是意味着放弃其他的事，其象征着可能性的限制，而人的可能性越是受限，越是接近死亡，死亡意味着所有事情都失去可能性，这是对个体独特性信念的巨大挑战；而且决定使个体进入到对自身存在的觉察，使个体意识到自己创造自己的世界，无依无靠的焦虑又挑战了终极拯救者的信念，令其陷入存在性的孤独中。在意识层面上，治疗师需要对病人的可能性进行有逻辑的系统分析，让病人想象各种选择可能伴随的后果，增强病人的掌控感，最重要的是最终的决定一定要由病人自己做出。 总的来说，整个治疗的过程可以拆解为：对责任的觉察→激发愿望→做出决定→付诸行动。每一个节点都有其受阻的形式，心理治疗的目的就是突破这些阻碍，把病人带到能够做出自由选择的境地。 4. 孤独 存在主义的孤独与常说的人际孤独有所不同，即使一个人在人际关系上并不孤独，存在孤独也不会消失，这是一种个体和其他生命之间无法跨越的鸿沟，是一种更基本的隔绝，即个体和世界的隔绝。这种存在孤独有下面几种表现方式： 死亡带来的孤独，没有人可以与别人一起死亡，没有人可以代替另一个人死亡，死亡是最孤独的人类体验。 自由带来的孤独，一个人为自己的生命负责，做自己生命的主人，不再相信有人可以创造或保护自己，不得不赤裸裸地面对存在。 成长带来的孤独，人类幼期的生存依赖于周围成人，在成长的过程中才逐渐建立自身边界，成为自立的、和他人分离的人，放弃与他人共生融合的状态就意味着面对存在孤独以及伴随的恐惧和无助感。 存在孤独会导致非常不舒服的主观状态，人们用以对抗存在孤独的主要力量就是关系。关系可以减缓孤独感但不能消除孤独感，如果我们承认自己的存在是孤独的，并毅然地面对孤独，就能理解他人，把他人当作和自己一样有感情、是同样孤独、同样害怕、同样试图让世界显得不那么诡异的人。否则，我们就会以对待工具一样的态度去对待他人，将他人作为否认孤独的工具，从而演化出各种心理病理： 活在他人眼中，通过寻求关系来逃避对处于存在核心的孤独感和空虚感的觉察，借着他人的重视而觉得自己的存在获得了肯定，他人成为被需要的工具，而不是被爱的个体。 融合，弱化自我边界，成为比自己优越的另一个人或群体（团体、使命、国家、项目等）的一部分，消除自己作为独立个体的觉察从而消除孤独感。 沉迷于性，性可以用于抑制死亡焦虑，人仿佛是被性欲所捕获和驱使，从而抵御了个体对自由和焦虑的意识。 害怕存在孤独的个体是最迫切寻求关系来获得帮助的人，但是他们也是最无力建立真诚的关系的人，他们所建立的关系是基于求生存，而不是求发展。病人必须认识到关系是不能消除存在孤独的，治疗师需要帮助病人以合适的剂量来面对孤独，比如让病人自行实施一段孤独期，并写下自己的想法和感受。能够面对和探索自身孤独的人能够学会以成熟的爱和他人建立关系，而只有那些能够真正和他人建立关系，已经达到某种程度的成熟的人，才能够忍受孤独。 5. 无意义 我们面对的是两个真实但又相互矛盾的命题： 人追寻意义。没有意义、目标、价值的生活会带来巨大痛苦，我们需要某种绝对性，需要可以让我们去努力追求、指导我们生活方向的实实在在的理想。 唯一的绝对就是没有绝对。存在主义把世界看作偶然的，人构成了自身、自己的世界以及在这个世界中自身所处的情境，在宇宙中没有宏大的设计、没有指导生活的原则，不存在意义。 因此我们要解决的问题是需要意义的个体如何在一个没有意义的世界中找到意义。人会追问两种意义，第一种是普遍意义，即生命总的来说是否有意义；第二种是世俗意义，即自己的生命的意义是什么。普遍意义暗示在个人之外、超越个人之上，存在某种设计，指向某种奇特的宇宙秩序，比如西方的宗教传统认为世界已经被上帝设计好了，个体的意义就是实现上帝的旨意，但随着科学的迅猛发展，人们越来越怀疑某种超越人类的存在，也越难接受普遍意义的系统。放弃普遍意义的系统，就需要以世俗意义作为替代物，一些常见的世俗活动都能给人带来生活目的感，它们看起来是正确美好的，并提供了内在的满足，不需要别的动机来支持： 利他。给世界留下较好的居住环境、服务他人、参与慈善，许多人把利他看为先验的真理，即便是反对其神学成分的人。 为理想奉献。理想使个体超越自我，成为更大架构中参与协作的一部分，而且很多理想或多或少都存在一点“利他”的成分。 创造。创造某种从未出现过的、美丽的、和谐的东西，也可以有效地消解无意义感，创造引出了新的存在，因此没有必要再去问为了什么，创造就是存在的理由。 享乐主义。完全地品尝生活，对生命的奇迹保持惊奇，把自己投入生命的自然韵律中，在最深的感触中寻找快乐。 自我实现。相信人类必须力争实现自我，应该献身于实现自己的潜能。 自我超越。前三种意义反映出超越自身利益、为某种外在于或高于自己的人或事而努力的渴望，弗兰克尔认为人只有错失生命意义时才会全神贯注于自身，而完全自我实现的人才会献身于超越自我的目标。 如果个体不能发现意义或者是对上面提到的各种世俗意义都不满意，深陷于无意义感，就会产生各种临床病理现象： 存在的空虚与存在性神经症。存在的空虚是一种无聊、冷漠和空虚的主观状态，个体感到愤世嫉俗、缺少方向感、质疑大部分活动的意义。这种空虚可能导致更显著的神经症症状，可能会表现为任何一种临床上的神经症，包括酗酒、抑郁、强迫性思维等，其行为模式反映了无意义感的危机。 十字军主义（冒险主义）。强烈地倾向于寻找宏大的和重要的理想并为之献身，没有任何区分地寻求各种理想目标，免得自己陷入无意义感中。 虚无主义。总是在怀疑其他人认为有意义的活动，其精力和行为均源自绝望，却又常常被伪装成是对生活的高度清醒和成熟态度。 无所谓。最极端程度的缺乏目的，既不强迫性地从理想中寻求意义，也不愤怒地抨击别人的人生意义，人陷入严重的无目标和冷漠状态。 当病人出现临床症状，并对生命意义进行追问时，一般是假设生命有一种意义，而自己没法找到这个意义，这种假设和存在主义的基本观点是矛盾的。人们抱有这种假设是希望以一种规律来归纳解释日常生活中的刺激和事件，因此意义的意义之一就是降低焦虑，减轻人在面对缺乏规律和结构的人生和世界时所产生的焦虑。一般说来，意义相关的问题并不纯粹，其他和终极关怀有关的问题会附加在其上，从而混淆意义的问题。比如一些人追求意义是为了超越死亡，希望在死后留下有价值的东西，而人们一想到所有东西都会灭亡，又会开始怀疑生命的意义。不但死亡焦虑常常伪装成缺乏意义，对自由和孤独的觉察带来的焦虑也常常和缺乏意义的焦虑相混淆。把存在看作是某种更宏大的、外在的设计的一部分，人在其中分配到某种角色，这是一种否认人对自己生活拥有自由、负有责任的态度，人用它来逃避无根感。害怕绝对的孤独也促使人向外寻求认同感，成为更大的团体的一部分，或是献身于某种理想，这些都是否认孤独的有效方式。治疗很重要的第一步就是重新描述病人的无意义主诉以便发现混淆在其中的其他问题，无意义体验可能只是一个替代品。 参与是缺乏意义的主要治疗答案。一些人缺乏意义是因为他把自己从生活中抽离，成为在宇宙视角下观察生命的疏离旁观者，所有事情都变得渺小又愚蠢。在绝大多数情况下，一个更广阔和全面的视角通常让观察者更客观，但另一方面这个视角又让生命活力枯竭，这种宇宙视角在逻辑上无懈可击，使其成为治疗中难以处理的问题。从逻辑上来说，“没有什么重要”这种观点是矛盾的，因为他意味着“没有什么重要”这种观点本身也不重要。其次，只有当人处在宇宙视角时，事物才显得没有意义，而这种时候只能占据我们部分的生活而已。参与生活、跃入承诺和行动之中可以消解宇宙观点固有的无意义感，人对意义的疑问总是要多于人对意义问题的回答，意义就像快乐一样不能直接求得，而是参与生活的副产品，参与生活不能从逻辑上反驳宇宙观点，但是它能让这些问题变得不再重要。总的来说，在治疗无意义感时，第一步是去分析和重新定义这个问题，找出是否与其他终极关怀有所关联，第二步是移除病人参与生活的障碍，找到是什么让病人无法去爱、无法获得满足和乐趣、无法直视自己的欲望。 死亡、自由、孤独三种终极关怀的问题都必须直接处理，但是面对无意义问题时，有效的治疗只能帮助病人将视线从这个问题上转移，接受参与生活的解决之道，而不是沉浸在无意义感的问题之中。人必须让自己沉浸在生活的洪流之中，让疑问随水流逝。","tags":["读书"],"categories":["笔记"]},{"title":"DDPM扩散模型的数学推导","path":"/2025/10/22/DDPM扩散模型的数学推导/","content":"因为《数字媒体导论》课程要做文献汇报，同组同学选择了和扩散模型相关的论文，而我又没接触过扩散模型，正好学习并整理一下DDPM模型的推导过程。 1. DDPM 扩散模型（Diffusion Model）是一种基于概率生成模型的深度学习方法，核心通过前向扩散（逐步加噪）和反向扩散（逐步去噪）两个对称的马尔可夫链，实现从随机噪声到真实数据的生成。 DDPM是扩散模型中的代表性工作，来自2020年发表的《Denoising Diffusion Probabilistic Models》： https://arxiv.org/abs/2006.11239https://arxiv.org/abs/2006.11239 1.1 前向扩散 前向扩散的目的是从真实数据 x0x_0x0​出发，按固定规则逐步添加高斯噪声，最终得到纯噪声 xtx_txt​（t为总步数）。前向扩散的核心是缓慢加噪，确保每一步噪声强度可控，且最终 xtx_txt​ 近似标准高斯分布 N(0,I)\\mathcal{N}(0, I)N(0,I)，加噪过程如下： q(xt∣xt−1)∼N(1−βt⋅xt−1, βtI)q(x_t | x_{t-1}) \\sim \\mathcal{N}(\\sqrt{1-\\beta_t}\\cdot x_{t-1},\\,\\beta_t I) q(xt​∣xt−1​)∼N(1−βt​​⋅xt−1​,βt​I) 即对于xt−1x_{t-1}xt−1​中的每个像素，以其像素值乘以1−βt\\sqrt{1-\\beta_t}1−βt​​为均值，βt\\beta_tβt​为方差，按照正态分布来随机采样一个值作为下一步图像的像素值。其中III是单位矩阵；βt∈(0,1)\\beta_t \\in (0, 1)βt​∈(0,1)为预定义的噪声强度，通常设定为从 0.0001 到 0.02 的递增序列（前期加少量噪声，后期加大量噪声，避免数据过早失真），从x0x_0x0​开始重复以上操作ttt次则可以得到噪声图片。 但是这样的循环操作会非常耗时，为了简化计算，可以采用重参数化技巧来将ttt次操作转变为一次操作，将(1)式利用标准正态分布来展开： xt=1−βt⋅xt−1+βt⋅ϵt−1ϵ∼N(0,1)x_t = \\sqrt{1-\\beta_t}\\cdot x_{t-1} + \\sqrt{\\beta_t} \\cdot \\epsilon_{t-1} \\quad\\qquad \\epsilon \\sim \\mathcal{N}(0, 1) xt​=1−βt​​⋅xt−1​+βt​​⋅ϵt−1​ϵ∼N(0,1) 即当前图像等于上一步图像和一个高斯噪声项的加权和，定义α=1−β\\alpha = 1 - \\betaα=1−β，再将xt−1x_{t-1}xt−1​递推展开： xt=αt⋅xt−1+1−αt⋅ϵt−1=αt⋅(αt−1⋅xt−2+1−αt−1⋅ϵt−2)+1−αt⋅ϵt−1=αtαt−1⋅xt−2+αt(1−αt−1)ϵt−2+1−αtϵt−1x_t = \\sqrt{\\alpha_t}\\cdot x_{t-1} + \\sqrt{1-\\alpha_t}\\cdot \\epsilon_{t-1}\\\\ = \\sqrt{\\alpha_t}\\cdot (\\sqrt{\\alpha_{t-1}}\\cdot x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\cdot \\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\cdot \\epsilon_{t-1}\\\\ =\\sqrt{\\alpha_t \\alpha_{t-1}}\\cdot x_{t-2} + \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} xt​=αt​​⋅xt−1​+1−αt​​⋅ϵt−1​=αt​​⋅(αt−1​​⋅xt−2​+1−αt−1​​⋅ϵt−2​)+1−αt​​⋅ϵt−1​=αt​αt−1​​⋅xt−2​+αt​(1−αt−1​)​ϵt−2​+1−αt​​ϵt−1​ 我们知道，将两个正态分布相加，最后仍然是正态分布，所得正态分布的方差为两者之和，那么对于上式最后的两个高斯噪声项，将它们相加可以用一个单个高斯噪声来代替： αt(1−αt−1) ϵt−2∼N(0, αt−αtαt−1)1−αt ϵt−1∼N(0, 1−αt)αt(1−αt−1)ϵt−2+1−αtϵt−1∼N(0, 1−αtαt−1)xt=αtαt−1 xt−2+1−αtαt−1 ϵ\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\,\\epsilon_{t-2} \\sim \\mathcal{N}(0, \\,\\alpha_t -\\alpha_t\\alpha_{t-1})\\\\ \\sqrt{1-\\alpha_t}\\,\\epsilon_{t-1} \\sim \\mathcal{N}(0, \\,1-\\alpha_t)\\\\ \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\sim \\mathcal{N}(0, \\,1-\\alpha_t\\alpha_{t-1}) \\\\ x_t = \\sqrt{\\alpha_t \\alpha_{t-1}}\\, x_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\,\\epsilon αt​(1−αt−1​)​ϵt−2​∼N(0,αt​−αt​αt−1​)1−αt​​ϵt−1​∼N(0,1−αt​)αt​(1−αt−1​)​ϵt−2​+1−αt​​ϵt−1​∼N(0,1−αt​αt−1​)xt​=αt​αt−1​​xt−2​+1−αt​αt−1​​ϵ 我们不断地进行递推展开至x0x_0x0​，最后就可以得到如下式子： xt=αtˉ x0+1−αtˉ ϵtx_t = \\sqrt{\\bar{\\alpha_t}}\\,x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\,\\epsilon_t xt​=αt​ˉ​​x0​+1−αt​ˉ​​ϵt​ 其中αtˉ=Πi=1tαi\\bar{\\alpha_t}=\\Pi_{i=1}^{t}\\alpha_iαt​ˉ​=Πi=1t​αi​，由此我们无需从 x0x_0x0​ 逐步计算到 xtx_txt​，可直接计算任意 t 步的加噪数据，为后续训练提供便捷。 1.2 反向扩散 反向扩散是扩散模型的核心学习部分，目标是从 xtx_txt​ 出发，逐步推断 xt−1,…,x0x_{t-1}, \\dots, x_0xt−1​,…,x0​，即计算p(xt−1∣xt)p(x_{t-1} \\mid x_t)p(xt−1​∣xt​)，但是这个分布我们无法直接计算，因此DDPM利用马尔可夫链的性质来对其进行转化。马尔可夫链中，时间步t的随机变量仅依赖于前一个随机变量，而与更前面的随机变量无关，故有p(xt−1∣xt)=p(xt−1∣xt,x0)p(x_{t-1}|x_t)=p(x_{t-1}|x_t, x_0)p(xt−1​∣xt​)=p(xt−1​∣xt​,x0​)，后者意思是在xtx_txt​和x0x_0x0​同时发生的情况下xt−1x_{t-1}xt−1​发生的概率。 对于p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)可以利用贝叶斯公式进一步展开。根据贝叶斯公式P(A∣B,C)=P(B∣A,C)P(A∣C)P(B∣C)P(A|B,C) = \\frac{P(B|A,C)P(A|C)}{P(B|C)}P(A∣B,C)=P(B∣C)P(B∣A,C)P(A∣C)​，将p(xt−1∣xt,x0)p(x_{t-1} \\mid x_t, x_0)p(xt−1​∣xt​,x0​) 代入这个公式则可以得到： p(xt−1∣xt,x0)=p(xt∣xt−1,x0)p(xt−1∣x0)p(xt∣x0)p(x_{t-1}|x_t,x_0) = \\frac{p(x_t|x_{t-1},x_0)p(x_{t-1}|x_0)}{p(x_t|x_0)} p(xt−1​∣xt​,x0​)=p(xt​∣x0​)p(xt​∣xt−1​,x0​)p(xt−1​∣x0​)​ 公式右边由三个概率分布组成，对于p(xt∣xt−1,x0)p(x_t|x_{t-1},x_0)p(xt​∣xt−1​,x0​)，再次利用马尔可夫链的性质，将其转换为p(xt∣xt−1)p(x_t|x_{t-1})p(xt​∣xt−1​)，则有 p(xt−1∣xt,x0)=p(xt∣xt−1)p(xt−1∣x0)p(xt∣x0)p(x_{t-1}|x_t,x_0) = \\frac{p(x_t|x_{t-1})p(x_{t-1}|x_0)}{p(x_t|x_0)} p(xt−1​∣xt​,x0​)=p(xt​∣x0​)p(xt​∣xt−1​)p(xt−1​∣x0​)​ 如此，右式的三个概率分布都是我们已知的正态分布，p(xt∣xt−1)p(x_t|x_{t-1})p(xt​∣xt−1​)是前向过程中的一步，而另外两项则可以用之前提到的重参数化的公式xt=αtˉ x0+1−αtˉ ϵtx_t = \\sqrt{\\bar{\\alpha_t}}\\,x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\,\\epsilon_txt​=αt​ˉ​​x0​+1−αt​ˉ​​ϵt​ 来给出它们的分布： p(xt∣xt−1)=N(αtxt−1, βtI)p(xt−1∣x0)=N(αˉt−1x0, (1−αˉt−1)I)p(xt∣x0)=N(αtˉx0, (1−αtˉ)I)p(x_t|x_{t-1}) = \\mathcal{N}(\\sqrt{\\alpha_t}x_{t-1},\\, \\beta_tI)\\\\ p(x_{t-1}|x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t-1}}x_0,\\, (1-\\bar{\\alpha}_{t-1})I) \\\\ p(x_t|x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha_t}}x_0,\\, (1-\\bar{\\alpha_t})I) p(xt​∣xt−1​)=N(αt​​xt−1​,βt​I)p(xt−1​∣x0​)=N(αˉt−1​​x0​,(1−αˉt−1​)I)p(xt​∣x0​)=N(αt​ˉ​​x0​,(1−αt​ˉ​)I) 接下来运用正态分布的乘法和除法公式即可给出p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)的分布，正态分布乘法和除法公式如下： N(μa, σa2)⋅N(μb, σb2)∝N(μc, σc2)1σc2=1σa2+1σb2μcσc2=μaσa2+μbσb2\\mathcal{N}(\\mu_a,\\, \\sigma_a^2)\\cdot\\mathcal{N}(\\mu_b,\\, \\sigma_b^2)\\propto\\mathcal{N}(\\mu_c,\\,\\sigma_c^2)\\\\ \\frac{1}{\\sigma_c^2}=\\frac{1}{\\sigma_a^2}+\\frac{1}{\\sigma_b^2} \\qquad \\frac{\\mu_c}{\\sigma_c^2}=\\frac{\\mu_a}{\\sigma_a^2}+\\frac{\\mu_b}{\\sigma_b^2} N(μa​,σa2​)⋅N(μb​,σb2​)∝N(μc​,σc2​)σc2​1​=σa2​1​+σb2​1​σc2​μc​​=σa2​μa​​+σb2​μb​​ N(μa, σa2)N(μb, σb2)∝N(μc, σc2)1σc2=1σa2−1σb2μcσc2=μaσa2−μbσb2\\frac{\\mathcal{N}(\\mu_a,\\, \\sigma_a^2)}{\\mathcal{N}(\\mu_b,\\, \\sigma_b^2)}\\propto\\mathcal{N}(\\mu_c,\\,\\sigma_c^2)\\\\ \\frac{1}{\\sigma_c^2}=\\frac{1}{\\sigma_a^2}-\\frac{1}{\\sigma_b^2} \\qquad \\frac{\\mu_c}{\\sigma_c^2}=\\frac{\\mu_a}{\\sigma_a^2}-\\frac{\\mu_b}{\\sigma_b^2} N(μb​,σb2​)N(μa​,σa2​)​∝N(μc​,σc2​)σc2​1​=σa2​1​−σb2​1​σc2​μc​​=σa2​μa​​−σb2​μb​​ 将三个正态分布代入以上公式，最后可以得到： p(xt−1∣xt,x0)=N(αˉt−1βt1−αˉtx0+αt(1−αˉt−1)1−αˉtxt, (1−αˉt−1)βt1−αˉt)p(x_{t-1}|x_t,x_0) = \\mathcal{N}(\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}x_0+\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t,\\,\\frac{(1-\\bar{\\alpha}_{t-1})\\beta_t}{1-\\bar{\\alpha}_t}) p(xt−1​∣xt​,x0​)=N(1−αˉt​αˉt−1​​βt​​x0​+1−αˉt​αt​​(1−αˉt−1​)​xt​,1−αˉt​(1−αˉt−1​)βt​​) σ2\\sigma^2σ2中的系数是由我们在前向过程中定义的，因此σ2\\sigma^2σ2可以直接计算，而在μ\\muμ中出现了项x0x_0x0​，但是在反向过程中，x0x_0x0​是不给出的，因此利用前面的重参数化公式xt=αtˉ x0+1−αtˉ ϵtx_t = \\sqrt{\\bar{\\alpha_t}}\\,x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\,\\epsilon_txt​=αt​ˉ​​x0​+1−αt​ˉ​​ϵt​反解出x0x_0x0​： x0=xt−1−αˉtϵtαˉtx_0 = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}} x0​=αˉt​​xt​−1−αˉt​​ϵt​​ 代入μ\\muμ中可得： μ=1αt(xt−1−αt1−αˉtϵt)\\mu = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_t) μ=αt​​1​(xt​−1−αˉt​​1−αt​​ϵt​) 如此，整个μ\\muμ中只剩下一个未知项ϵt\\epsilon_tϵt​，因为这个噪声是在前向过程中生成的，反向过程中不能获取这个噪声的值，因此我们可以让模型来预测这个ϵt\\epsilon_tϵt​的值，再根据公式来得到xt−1x_{t-1}xt−1​的分布，如此循环直到我们计算出x0x_0x0​，其流程如下所示：","categories":["deep-learning"]},{"title":"记一次pytorch Dataset性能优化","path":"/2025/10/16/记一次pytorch Dataset性能优化/","content":"最近跑模型的时候发现GPU利用率奇低，基本维持在百分之十几左右，还会间歇性掉到0，导致训练60K个iteration就需要接近3天的时间，而官方论文的训练总数是整整600K iteration，这就意味着如果我要跑一次全量训练起码要30天，这肯定是无法接受的，因此需要定位问题再进行性能优化。 1. 问题定位 PyTorch Profiler是PyTorch 1.8+内置的全栈性能分析器，可一键记录CPU、GPU、内存、数据传输、算子调用栈等细粒度指标，并直接输出TensorBoard 可视化或Chrome Trace文件，帮助快速定位训练/推理瓶颈。 根据官方文档，将训练代码放到以下代码之内： with profile( activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], schedule=torch.profiler.schedule(wait=1, warmup=1, active=4, repeat=1), on_trace_ready=tensorboard_trace_handler(./train_log), with_stack=True, profile_memory=True, record_shapes=False, ) as prof: # ...训练代码... prof.step() if iteration = 1 + 1 + 4: return 运行之后就能在同级目录里看到trace.json文件，然后安装tensorboard的profiler插件： pip install torch_tb_profiler 使用tensorboard打开trace文件： tensorboard --logdir=./train_log 在面板中可以看到各种操作的时间占比，但是不知道为什么我这里看不到Dataloader的时间占比，即便我按照某issue里面说的把Dataloader的num_workers设置为0，使数据加载发生在主线程里，依然没法解决这个问题。无论如何，从面板可以看出GPU利用率只有11.85%，Kernel操作只占了运行时间的11.3%，性能亟待优化。 然后我在profiler_tutorial的warning里看到tensorboard和profiler的集成已经弃用了，可以使用Perfetto UI来分析trace文件，用其打开trace文件终于能够显示Dataloader的数据了，根据分析结果可以看到在一个iteration中Dataloader的运行时间达到了4s左右，模型的forward方法在900ms左右，backward则在1.4s左右，很明显性能瓶颈在于数据加载，需要针对Dataset类进行优化。 2. 性能优化 2.1 转换为LMDB数据库 LMDB是一个高性能、嵌入式键值数据库，被广泛用于需要低延迟、高并发读的场景，LMDB整库就是单个文件，通过 mmap() 一次性映射到进程地址空间，可以减少文件寻址的时间。网上大多博客在谈数据加载优化时首要提到的就是将数据集转换为lmdb格式，故采用如下代码进行转换，在转换时，我把图片转换为了tensor bytes再进行存储，读取时就可以使用torch.load直接转换为tensor，而不需要再进行图片解码，但是要注意存储的tensor是uint8类型的，如果存储float32则会导致占用空间大大增多，增加的io耗时会多于减少的解码耗时，得不偿失： for i, (key, img_path) in enumerate(tqdm(img_meta, desc=转换并写入 LMDB)): try: img = Image.open(img_path).convert(RGB) np_img = np.array(img) # - (H, W, C), uint8 tensor = torch.from_numpy(np_img) # - (H, W, C), uint8 tensor = tensor.permute(2, 0, 1).contiguous() # - (C, H, W), uint8 tensor_bytes = tensor.numpy().tobytes() # 存入 LMDB txn.put(key.encode(utf-8), tensor_bytes) except Exception as e: print(f 处理文件 img_path 时出错: e) continue # 跳过损坏的文件 if (i + 1) % write_frequency == 0: txn.commit() txn = env.begin(write=True)txn.commit()env.close() 经过测试，LMDB确实加快了数据读取，但是dataloader依旧是瓶颈，还需要进一步优化。torch profiler的trace可以直观地看出瓶颈在哪，但是要具体到各行代码的耗时分析时就不够好用了，因此下面转用line_profile来进行分析。line_profiler可以给出各行代码的时间占比、执行次数等信息，直接用pip安装即可，然后在Dataset类的__get_item__方法上添加@profile装饰器，使用kernprof -l -v test.py命令来启动一个迭代读取Dataset的代码，最终会在终端里打印各行代码耗时信息，把代码里耗时比较高的挑出来，结果如下： Line # Hits Time Per Hit % Time Line Contents============================================================== 135 3030 11093407.8 3661.2 9.3 hr_img_bytes = self._get_data_from_lmdb(self.img_env, hr_img_key) 139 3030 24507465.6 8088.3 20.5 lr_img = torch.load(io.BytesIO(lr_img_bytes)).float() / 255.0 140 3030 32407331.8 10695.5 27.1 hr_img = torch.load(io.BytesIO(hr_img_bytes)).float() / 255.0 156 3030 12741764.0 4205.2 10.7 hr_depth_bytes = self._get_data_from_lmdb(self.depth_env, hr_depth_key) 行号135的代码用于从LMDB数据库中读取高分辨率图片的bytes数据，行号156则是读取高分辨率图对应的深度图，行号139-140则是将bytes数据转换为低分辨率图片和高分辨率图片，然后转换数据类型并进行归一化。这四行代码的耗时占比基本都在10%以上，存在可优化空间。 同时使用代码从Dataset中读取100个batch，整体耗时达到了119.64s，平均1.2s读取一个batch。 2.2 div_() 针对lr_img = torch.load(io.BytesIO(lr_img_bytes)).float() / 255.0这行代码，我原以为其瓶颈在于torch.load操作，但是当我把这行代码分为torch.load、lr_img = lr_img.float()和lr_img = lr_img / 255.0时才发现最后这个除法操作的耗时远远大于前两步。 经过调研发现，lr_img = lr_img / 255.0是需要先开辟一块新内存用于存储结果，然后再进行除法操作的，而pytorch中提供了div_()方法来进行原地除法，无需开辟新内存，使用以下代码来比较两者的性能差异： import torch, timex = torch.randn(3, 720, 1280, dtype=torch.float32)# 1. out-of-placetorch.cuda.synchronize()t0 = time.time()for _ in range(100): y = x / 255.0torch.cuda.synchronize()print(out-of-place:, time.time() - t0, s)# 2. in-placetorch.cuda.synchronize()t0 = time.time()for _ in range(100): x.div_(255.0)torch.cuda.synchronize()print(in-place:, time.time() - t0, s) 最终结果如下，两者性能差距将近21倍： out-of-place: 0.12485671043395996 sin-place: 0.005963802337646484 s 再次使用line_profiler进行分析，除法操作的时间占比大幅降低： Line # Hits Time Per Hit % Time Line Contents============================================================== 134 3030 1165666.1 384.7 2.5 lr_img = torch.load(io.BytesIO(lr_img_bytes)) 135 3030 2003618.1 661.3 4.3 hr_img = torch.load(io.BytesIO(hr_img_bytes)) 138 3030 1288034.8 425.1 2.7 lr_img = lr_img.float() 139 3030 290921.0 96.0 0.6 lr_img.div_(255.0) 140 3030 1087394.6 358.9 2.3 hr_img = hr_img.float() 141 3030 195675.0 64.6 0.4 hr_img.div_(255.0) 再次测试读取100个batch的时间，总共耗时46.88s，提速相当明显。 2.3 顺序读 在原来的__get_item__函数中，对于低分辨率图片和高分辨率图片是交替读取的： for i, img_path_dict in enumerate(img_seq): lr_img_path = img_path_dict[lr_img_path] hr_img_path = img_path_dict[hr_img_path] # 将绝对路径转换为相对路径作为key lr_img_key = os.path.relpath(lr_img_path, self.img_root) hr_img_key = os.path.relpath(hr_img_path, self.img_root) # 从LMDB获取图片数据 lr_img_bytes = self._get_data_from_lmdb(self.img_env, lr_img_key) hr_img_bytes = self._get_data_from_lmdb(self.img_env, hr_img_key) # 加载 lr_img = torch.load(io.BytesIO(lr_img_bytes)).float() hr_img = torch.load(io.BytesIO(hr_img_bytes)).float() lr_img.div_(255.0) hr_img.div_(255.0) 而前面提到，LMDB是使用B+树来组织数据的，我们都知道，B+树的相邻叶子节点之间存在指针，从而优化了顺序访问的速度，而高分辨率图片和低分辨率图片的key是不一样的，高分辨率的key之间是顺序的，低分辨率的key之间也是顺序的，而上面这种读取一张低分辨率图片又去读取一张高分辨率图片的做法就无法享受到B+树的优化了。 因此我把低分辨率图片和高分辨率图片的读取聚在了一起： with self.img_env.begin(write=False) as txn: for i, img_path in enumerate(img_seq): lr_img_path = img_path[lr_img_path] lr_img_key = os.path.relpath(lr_img_path, self.img_root) lr_img_bytes = txn.get(lr_img_key.encode(utf-8)) lr_img = torch.load(io.BytesIO(lr_img_bytes)).float() lr_img.div_(255.0) lrs.append(lr_img) for i, img_path in enumerate(img_seq): hr_img_path = img_path[hr_img_path] hr_img_key = os.path.relpath(hr_img_path, self.img_root) hr_img_bytes = txn.get(hr_img_key.encode(utf-8)) hr_img = torch.load(io.BytesIO(hr_img_bytes)).float() hr_img.div_(255.0) hrs.append(hr_img) 再次测试100个batch读取时间，用时22.056s。 3. Final 除了以上的优化以外，我发现在训练过程中模型并不需要用到高分辨率图的深度图，因此删掉了读取代码，也带了很大的速度提升，不过没什么技术性，在此不赘述。 使用优化后的数据集再跑一次pytorch profiler分析，最终结果如下图所示： 可以看到相比模型推理和反向传播，数据集加载要短得多。然而跑训练时虽然能够感受到一点提速，但是GPU利用率也仅仅是稳定在50%左右，还不是很理想，目前仍在想办法优化其他部分的代码。","categories":["deep-learning"]},{"title":"Raft算法详解","path":"/2023/11/05/Raft算法详解/","content":"Raft算法用于将一台服务器的状态机分布到服务器集群中，保持各个服务器的状态一致，以便在主服务器宕机时能够找出一台新服务器替代，并且让用户无法察觉到这个过程。Raft是一种共识算法，可以将其主要过程分为领导者选举、日志复制、日志压缩等，通过Raft算法，我们可以实现具有容错的分布式系统。本文详细介绍了Raft算法的实现原理，并且附带详细的案例分析帮助理解。 1. Raft的基本概念 Raft是一种共识算法，只有在大多数(多于半数，⌊n2⌋+1\\lfloor\\frac{n}{2}\\rfloor+1⌊2n​⌋+1)服务器同意某一个命令并保存了该命令之后，这个命令才会被提交，然后服务器再对客户端进行回应，这样做是为了保证当主服务器宕机时，能够找出一台和原来主服务器具有相同状态机的替代服务器。 为了实现上述的“大多数”原则，Raft给服务器设置了三种状态，分别是领导者(leader)、跟随者(follower)和候选者(candidate)。跟随者通过投票选出领导者，只有得到“大多数”跟随者投票的服务器能成为领导者；领导者负责将命令同步给跟随者，只有被“大多数”跟随者确认的命令才能提交。 客户端发来的命令必须被保存，因为当有跟随者因为各种原因错过了某些命令时，领导者必须负责将跟随者缺少的命令发送给跟随者；当跟随者收到领导者发来的命令时，也必须根据自己保存的内容来判断传来的命令是重复的、是过时的、是可以确认的、还是缺少了一些前置命令的。 Raft算法将命令保存到日志当中，日志中保存的命令实际上可以构建出一个状态机，只要保存的日志与领导者一致，重启的服务器按照日志保存的命令顺序执行，最后就可以同步到和领导者相同的状态。领导者将自己的日志复制给其他的跟随者实际上就是将状态机复制给其他服务器。 2. 领导者选举 2.1 选举过程 Raft算法将时间划分成一个个任期(term)，当有跟随者发起选举时其转变成为候选者，然后将当前任期号加一，如果它得到了大多数跟随者的投票，那么它就可以由候选者转换成为当前任期的领导者。每个服务器需要维护一个变量来记录当前的任期号。 Raft使用超时机制来控制选举的发生。每个服务器被赋予一个随机的超时时间，当一个跟随者经过一个超时时间后还没有收到来自领导者的消息时，它会认为领导者已经宕机，并发起一次新的选举。领导者宕机或者领导者与跟随者网络不通都会导致这种超时的发生。 发起选举的服务器需要做三件事：1. 首先转换为候选者状态，投票给自己，2. 然后将保存的任期号加一，并把新的任期号发送给其他所有服务器。3. 如果得到大多数选票，转换成领导者状态；如果在一定时间内没有得到大多数选票，就重置超时时间并转回跟随者状态等待下一次选举。 一个服务器是否投票另一个服务器取决于双方的任期号。当其他服务器收到投票请求时，如果发来的任期号大于自己保存的任期号，则选择投票给该服务器并设置新的任期号，如果发来的任期号小于等于自己保存的任期号，则拒绝投票给该服务器。当该服务器获得大于半数的投票时，其从候选者状态转换为领导者状态。如果该服务器没有成为领导者，则重新回到跟随者状态并重置超时时间。 服务器有命令需要发送时就发送命令，没有命令需要发送时就定时发送“心跳”信息给所有跟随者。为了避免在领导者未宕机的情况下跟随者发起选举，领导者需要定时发送“心跳”信息，当跟随者收到来自领导者的消息(心跳信息或任何其他信息)时就会重置自己的超时计时器。领导者发送心跳信息的间隔时间要小于跟随者超时时间的最小值。领导者发送心跳信息时需要附带任期号，因为领导者可能已经过时了，而其他服务器需要通过任期号来判断这点，具体请参考2.2.3 多领导者。 2.2 corner case 2.2.1 Raft初启动 当Raft算法刚刚启动时，所有服务器都处于跟随者状态，由于没有领导者发送心跳信息，在一段时间过后必定有一个服务器首先超时并发起选举。 每个服务器的超时时间都是随机设定而不是设为相同的固定值，这是为了防止出现大多数服务器同时超时的情况。当所有服务器同时超时时，每个发起选举的服务器都会先投票给自己，这样就没有一个服务器能够得到大多数选票，然后又经过一段相同的时间，所有服务器再次同时发起选举，这样就会导致死循环发生。 2.2.2 多候选者 虽然随机设置的超时时间让同时超时变得不太可能，但是一个新任期中还是有可能出现两个及以上的候选者。比如当两个服务器的超时时间很接近时，一个服务器超时后另一个服务器马上就紧跟着超时了；又比如有一条心跳信息因为网络原因丢失了，那么就有一个跟随者的时钟没有被重置，其剩余的时间可能恰好与其他跟随者的超时时间很相近。 现在假设服务器A超时，将任期号升至2，且在还没来得及将投票请求发送出去时服务器B也超时，也将自己的任期号升至2，此时新任期中就有两个候选者，网络中包含了两个候选者的投票请求。其他跟随者可能在收到一个投票请求后又会收到另一条投票请求，但是2.1节中描述的投票规则保证了：一个服务器在一个任期中最多投票一次。 若另一个服务器C先收到了A的投票请求，C由于收到了更大的任期号所以会选择同意投票，然后其任期号也升至2。当其再收到B的投票请求时，由于收到的任期号还是2，并不比自己现在的任期号大，因此C会拒绝投票，同理，任何其他来自任期2的投票请求都会被C拒绝。 “大多数原则”加上一任期一票的限制保证了一个重要的性质：一个任期内只能产生一个领导者。如果有n个服务器，则当上领导者必须获得⌊n2⌋+1\\lfloor\\frac{n}{2}\\rfloor+1⌊2n​⌋+1的选票，而服务器在一个任期内只会投票一次，总投票数最多为n，如果出现了两个领导者，则总票数大于n，矛盾。 2.2.3 多领导者 需要注意：一个任期内只有一个领导者不等于某时刻下只有一个领导者。假设任期1的领导者与其他所有跟随者的网络连接断开，剩下的跟随者在超时后会选举出任期2的领导者，此时任期1的领导者和任期2的领导者同时存在，但并不违反一个任期内只有一个领导者的性质。 与其他服务器隔离的任期1领导者仍在不断尝试发送心跳信息，当它和任何一个服务器的网络连接恢复后，收到心跳信息的服务器会发现这是一个来自任期1的信息，说明存在一个已经过时的领导者，服务器会在回应中附带上最新的任期号，任期1领导者看到这个任期号后直到自己已不再是领导者，就会转换为跟随者。 2.2.3 无法选出领导者 如果大多数服务器宕机或者断网，此时没有服务器能够得到大多数选票，剩下的服务器就会不断地发起选举，然后失败，然后一段时间后继续发起选举，直到在线服务器的数量恢复到一定程度，在这种情况下会经历数个没有领导者的任期，这段时间内系统无法处理客户端的请求。 即使当大多数服务器宕机时当前任期的领导者没有宕机，它仍旧回应不了客户端的请求，因为它无法将这条命令添加到大多数跟随者的日志中。 3. 日志同步 3.1 同步过程 当领导者被选举出来后，它就可以开始接收客户端的请求。当接收到来自客户端的命令时，领导者需要负责将命令同步到所有跟随者的日志中，同步过程总共分为四步： 领导者首先将这条命令添加到自己的日志，然后发送给所有跟随者。 跟随者收到命令后也将这条命令加到自己的日志当中，然后向领导者确认。 当领导者收到大多数跟随者的确认之后才能提交并执行这条命令，执行完毕之后向客户端发送响应。如果领导者没有得到大多数跟随者的确认，领导者需要不断给未确认的跟随者重发命令直到收到确认。 在之后的心跳信息中，会包含有领导者已经提交到哪条命令的信息，收到心跳信息的跟随者查看该信息，如果自己的日志中包含有领导者已提交的命令，则跟随者也提交并执行这些命令。 服务器通过网络发送的任何消息都有可能会丢失，因此可能会有跟随者没有收到某些命令，如果此时领导者发来新的命令，跟随者需要拒绝这条命令并要求领导者将跟随者缺失的命令重新发送，否则跟随者的日志会缺少条目，这样就与领导者的日志出现了不一致。 为了避免日志条目的缺失，保存到日志中的条目还需要额外保存添加这个条目时所在的任期，结合条目在日志中的索引号和相对应的任期号，跟随者就能判断自己是否缺少了一些日志条目。保存了任期号的日志如下图： 领导者在发送新的日志条目给跟随者时，要附带上该条目的前一条目的索引和任期号，跟随者利用发来的索引从日志中取出条目，如果不存在条目或者条目记录的任期和传来的任期不符，那么跟随者就拒绝添加新的日志条目。实际上这是在确认新条目之前的条目与领导者的是否一致。 由于每个跟随者成功接收的情况不同，因此领导者需要记录每个跟随者的日志同步情况。领导者维护一个matchIndex数组和一个nextIndex数组，前者保存各个跟随者的日志和自身日志匹配到了哪一个索引，后者保存下一次要发送给各个跟随者的日志条目的索引号。 在上图的例子中，原本的nextIndex[1] = 4，因此领导者给跟随者1发送了索引为4的日志条目，但是跟随者1由于缺少索引为3的条目所以拒绝添加，领导者收到拒绝消息后会将nextIndex[1]减一，即下一次发送日志条目3(为了提高效率，可以一次性发送条目3及其之后的所有条目)，如果继续失败则不断减小nextIndex直到成功，成功则将nextIndex[1]设置为发送的最后一个条目的索引号加一。 3.2 corner case 3.2.1 更换领导者 当服务器们选出新的领导者时，新领导者不知道各服务器的日志同步情况。假设原领导者宕机，剩下的服务器选出一个新的领导者，但新选出来的领导者由于尚未与跟随者们进行过日志同步，因此不知道其他服务器上日志的同步情况，即无法确定matchIndex和nextIndex数组的值。 为了能够进行正确的同步，新领导者将每个跟随者的matchIndex设置为0，nextIndex设置为自己日志中的最大索引加一。此时当领导者收到新的命令时，它就会直接发送最新的命令给其他跟随者，如果其他跟随者同意添加，那说明跟随者的日志和领导者的日志是完全一致的，可以将matchIndex设置为日志的最大索引；如果跟随者拒绝添加，则领导者像常规情况一样逐步减小nextIndex尝试逐个发送之前的条目，直到跟随者同意添加。 即使新领导者短时间内没有收到新的命令，也可以通过心跳信息来获取跟随者的日志同步情况。即使是心跳信息，领导者也要附带上prevIndex和prevTerm，领导者将这两个值分别设置为日志中最后一个条目的索引和任期号，如果收到了跟随者否定的响应则说明跟随者的日志没有完全和领导者的同步，然后领导者将相应的nextIndex减一，后续过程则是和前文一样的不断试探。 3.2.2 跟随者宕机 如果跟随者宕机，导致当前没有多于半数的服务器在线，那么剩下的服务器将无法提交任何一条命令。由于领导者无法得到大多数服务器的确认，因此不能将命令提交执行，当然也就不能对客户端进行响应，客户端长时间得不到响应，可能会将命令重发，领导者就会将同一条命令再次保存到日志中，后续大多数服务器重新上线后领导者可能会执行多条相同的命令，不过这些命令是具有幂等性的，因此不会对状态机造成影响。 3.2.3 条目未提交时领导者宕机 一个领导者可能在还未提交某条命令时就宕机了，但它可能之前已经将这条命令添加到了某些跟随者的日志中，这些未提交的日志条目要么被新的领导者提交，要么被新的领导者覆盖。 如果新领导者日志中没有未提交的日志条目，当它将新日志条目添加到其他跟随者日志中时会覆盖跟随者日志中的日志条目。当跟随者收到新条目时，如果发现新条目要插入的位置已有条目，跟随者需要用新条目覆盖旧条目，因为跟随者需要和领导者保持日志同步。 这里画了5台服务器，如果只有前3台服务器，第二台服务器不可能成为新领导者，这与即将引入的新选举规则有关 如果新领导者日志中有未提交的日志条目，它需要将这个条目先同步到跟随者日志中，但即使大多数服务器已经有了这个条目，它也要等提交了一条当前任期的日志条目后才能提交前一任期的日志条目。 如果不这样做的话，就有可能导致已提交的条目被下一个领导者覆盖。以原始论文中给出的图为例，在(a)中S1在提交索引2条目前宕机；(b)中S5成为任期3的新领导者，并在给自己的日志添加一条新条目后宕机；©中S1恢复并成为任期4的领导者，将索引2的条目添加给S3后立即提交该条目，然后再次宕机；(d)中S5成为任期5的领导者，并且将原来添加的条目同步到了所有跟随者的日志中，这样就导致了S1在索引2处已提交的条目被覆盖！ 但如果在©中S1没有立即提交索引2的条目，而是像(e)一样等到提交了任期4的条目后再提交任期2的条目，那么即使S5再次成为领导者也无法再覆盖跟随者的日志，因为S5会发送prevIndex = 1, prevTerm = 1，这已经不符合S1、S2、S3目前的日志状态，因此会拒绝添加这个条目。 又如果S1在还没提交任期4的条目时又宕机了，但此时任期2的条目还未被提交，是允许被覆盖的，因此也没有问题。 但事实上，在引入新的选举规则后，S5根本不可能再次成为新的领导者 3.2.4 缺少条目的跟随者成为领导者 按照我们目前的选举规则，一般谁先发出选举请求谁就更可能成为新的领导者，假如一个缺少条目的跟随者成为领导者，会导致其他跟随者已提交的条目被覆盖。为了防止这种情况的发生，需要在已有的选举规则上再新增一条：跟随者收到投票请求时，只有当候选者的日志比自己的日志更新的时候才同意投票。 Raft算法这样来定义“新”：1. 最后一个日志条目的任期号更大。2. 如果相同，则日志更长的为新。 在3.2.3的图(e)中，如果S1，S2，S3的日志中添加了任期4的条目，那么即使S1宕机，S5也不可能成为任期5的领导者了，S5可以得到S4的选票，但是得不到S2和S3的选票。通过添加这样一条新的选举规则，S5不可能再次覆盖其他服务器中已经提交了的日志条目。 为了让服务器在选举阶段能够相互对比日志情况，候选者在发送投票请求时不仅要附带自己所处的任期，还要附带自己最后一条日志条目的索引和任期号。跟随者用发来的索引和任期号对比自己的最后一条日志条目，如果候选者的更新则同意投票。 4. 快照 处理的用户请求越多，服务器的日志就越长，占用的内存空间就越多，因此需要使用快照技术来压缩日志。 当日志达到一定大小之后，服务器选定一个已经提交了的索引位，将该索引位下的服务器状态保存到硬盘当中，然后该索引位之前的日志条目就可以被丢弃。 保存快照时服务器还要记录快照中包含的最后一条日志条目的索引和任期号，当领导者需要发送快照后一条日志条目给其他跟随者时，要将prevIndex和prevTerm设置为快照中尾部条目的对应值。 如果跟随者缺少的条目已经被包含在了领导者的快照当中，即nextIndex值小于等于lastIncludedIndex值时，那么领导者就直接将保存的快照发送给跟随者，跟随者直接将服务器的状态设置为快照中的状态。 5. 总结 Raft算法从选举开始，当跟随者超时之后转为候选者发起选举，然后向其他跟随者发送自己的任期号、日志尾部条目的索引和任期号；跟随者收到投票请求后，如果对方的任期号大于自己的任期号、并且对方的日志更新，则同意投票。候选者得到大多数选票后成为领导者。 领导者负责处理客户端请求并将日志条目同步到其他跟随者的日志当中。当领导者收到客户端新请求后，要将新的日志条目发送给跟随者，并附带prevIndex、prevTerm，跟随者确定自己没有缺失之前的条目后确认添加，领导者得到大多数跟随者的确认后将命令提交。没有条目要发送的时候领导者向跟随者发送心跳信息避免跟随者发起选举。 当服务器的日志到达一定的大小之后，需要通过快照来进行压缩。快照保存了服务器的状态，还有包括的最后一条日志条目的索引和任期号，用于领导者进行日志同步。发送快照可以让一些落后很多的跟随者快速地来到和领导者接近的状态。","tags":["分布式系统"],"categories":["计算机基础"]},{"title":"《经济学原理》宏观经济学分册总结","path":"/2023/10/02/《经济学原理》宏观经济学分册总结/","content":"宏观经济学的研究范围是整体经济的运行情况，涉及到失业、通货膨胀和经济增长等。宏观经济学中介绍了很多基础概念，我认为其中最重要的一些知识点是：储蓄、投资、货币制度和通货膨胀，本文也将主要围绕这些概念来进行总结。 1. GDP与储蓄、投资 国内生产总值(GDP)衡量一个国家在某一既定时期生产的所有最终物品与服务的市场价值。GDP(用Y表示)可以分为四个部分：消费(C, consumption)，投资(I, investment)，政府购买(G, government purchase)，净出口(NX, net export)。 Y=C+I+G+NXY = C + I + G + NX Y=C+I+G+NX 其中，对于净出口有一个重要的事实：净出口总是必然等于资本净流出(net capital outflow，NCO，本国居民购买的外国资产减外国人购买的本国资产)。当一个本国人将物品出口到外国时，他就获得了相应数量的外国货币，当他使用外国货币购买外国资产时资本净流出就增加了，即使他到银行将这些外币兑成本国货币，银行同样会使用这些外币购买外国资产或者是兑给需要购买外国资产的人，因此净出口的增加与资本净流出的增加相当；当本国进口外国物品时，国外卖者同样也会用得到的货币购买本国的资产，净出口的减少与资本净流出的减少也相当。 我们将GDP恒等式进行移项后可以得到： Y−C−G=I+NXY - C - G = I + NX Y−C−G=I+NX 等式左边是在用于消费和政府购买后剩下的一个经济中的总收入，这个量就是国民储蓄，这个等式说明，国民储蓄等于投资和资本净流出。当国民将收入储蓄起来时，这些储蓄既可以用于为国内资本积累筹资，也可以用于为国外资本的购买筹资。 2. 储蓄、投资和金融体系 金融体系负责匹配一个人的储蓄和另一个人的投资，当一国的储蓄增多时，就有更多资源用于资本投资，而资本是用于生产各种物品与服务的，因此投资增加能够提高该国的生产率。 在金融市场中储蓄者可以直接给借款者提供资金，金融市场包括债券市场和股票市场。购买债券即向公司借款，到达期限后收获本金和利息，长期限和信用风险高的债券拥有更高的利息；购买股票就拥有了获得公司分红的权利，相比于债券拥有更高的收益，但如果公司陷入财务困境，需要先支付债券所有者应得的部分，因此股票也有更高的风险。 在在金融中介机构中储蓄者间接地向借款者提供资金。银行是最常见的金融中介机构，银行通过存款和贷款之间的利息差来获得利润。 为了分析储蓄与投资的关系，我们要分析可贷资金市场的供给与需求，前一节说过，国民储蓄等于投资和资本净流出，因此储蓄是可贷资金供给的来源，而投资和资本净流出是可贷资金需求的来源，利率则相当于是贷款的价格。和微观市场相似，当利率升高，储蓄的吸引力增大而借款的代价上升，就会使可贷资金供给上升、借款的投资减少，同时，由于利率上升，人们更倾向于持有本国资产而非外国资产，导致资本净流出减少，投资和资本净流出的减少一起导致了可贷资金需求的减少。 利率控制了供给与需求的平衡。 3. 可贷资金市场与外汇市场 外汇市场是关于本国货币的供需市场，当本国人需要购买外国资产时，他需要将本国货币兑成外币，因此他在外汇市场上供给了本国货币，同时增加了资本净流出，外汇市场的供给来自资本净流出；当外国人需要购买本国资产时也需要将外币兑成本国货币，他在外汇市场上需要本国货币，这增加了本国的净出口，外汇市场的需求来自净出口。 而决定外汇市场供需平衡的因素是汇率。当本国汇率上升时，外国买者一单位外币所能兑到的本国货币变少了，相当于本国商品变贵了，这就会使本国出口减少，同时由于一单位本国货币能兑的外币增加，本国的进口增加，因此汇率上升减少了净出口，本国货币的需求下降。但汇率并不影响资本净流出，正如前一节所述，资本净流出取决于利率，即使凭借着高汇率买到了更多的外国资产，买者最终还是要将赚到的外汇转换成本国货币，又会因为高汇率导致兑得的货币减少。 我们可以看到，在可贷资金市场中，资本净流出是可贷资金需求的来源，在外汇市场中，资本净流出是供给的来源。因此，当可贷资金市场的变动导致资本净流出变动时，也一定会影响到外汇市场。比如当可贷资金供给减少时，利率就会上升，同时导致资本净流出减少，资本净流出的减少就会导致外汇市场上的汇率升高。 4. 银行与货币制度 银行对于货币供给有着至关重要的作用，假设储户将100元存入银行，银行留下10元货币作为准备金以应对储户取款，然后将90元货币作为贷款发放出去，这样一来，储户手上拥有100元存款，贷款人手上有了90元通货，货币供给从原来的100元变成了现在的190元，看起来银行像是凭空创造了货币，但银行并没有创造出财富，只是让经济变得更加具有流动性。 而且这种情况不会就这样停止，当贷款人使用贷款购买物品或服务时，货币就转移到另一个人身上，这个人也可能将这笔钱存入银行，银行可以利用这部分货币继续发放贷款，再次创造货币来使这个过程延续下去。如果假设每个银行的准备金率(准备金占存款总数的比率)都等于0.1，那么我们只需要将等比数列{100∗0.9,100∗0.92,100∗0.93,...}\\{100*0.9, 100*0.9^2,100*0.9^3,...\\}{100∗0.9,100∗0.92,100∗0.93,...}求和即可得出银行用这100元创造出的货币供给量。 为了监管银行体系和调节经济中的货币量，各国都设置了中央银行机构。中央银行可以通过公开市场操作来控制货币量，当需要增加货币供给时，中央银行便在金融市场上从公众手中买入政府债券，这些货币有些被公众直接持有，有些会被公众存入银行，作为通货被持有的每一单位货币都增加了一单位货币供给，被存入银行的一单位货币则因银行的作用增加了大于一单位的货币供给。如果需要减少货币供给，中央银行则抛出政府债券。 除了公开市场操作，中央银行可以通过向其他银行贷款来增加经济中的准备金数量，通过这种贷款银行拥有了更多的准备金，增加的这些准备金可以让银行通过借贷手段来创造更多的货币，中央银行充当了银行的银行的角色。 5. 货币增长与通货膨胀 通货膨胀是经济中物价水平不断上升的情况，古典通货膨胀理论认为通货膨胀的发生是由货币价值下降引起的，货币价值越低，物价水平越高。而货币的价值同样也是由供给和需求决定的。 对于货币供给，我们假设这是一个可以由中央银行完全掌控的政策变量，而对于货币需求，货币需求反映了人们想以流动性形式持有的财富量，因为货币是购买物品与服务的交换媒介，所以当物价水平越高，交易时需要使用的货币量越多，则人们选择持有的货币越多，因此物价水平的上升增加了货币需求。在长期中，物价总水平会调整到使货币需求等于货币供给的水平。 当中央银行选择向经济中注入货币时，会让供给曲线向右移动，导致货币价值下降、物价水平上升。当货币注入到经济中时，经济中生产物品与服务的能力没有改变，古典理论将这些不按货币单位衡量的变量称为真实变量，而价格、收入等变量则被称为名义变量，认为货币作为名义变量其变动不影响真实变量的理论称为货币中性。货币量增多不会引起物品与服务的增多，则每单位物品与服务对应的货币量自然就上升了，从而物价水平就上升了。 6. 总需求供给模型与通货膨胀 总需求曲线衡量在任何一种既定的物价水平下经济中所有物品与服务的需求量，总供给曲线则衡量物品与服务的供给量，而物价水平控制二者的平衡。总需求供给曲线像是微观经济学中供给需求曲线的放大版，但前者的变动涉及各种宏观经济变量，后者则涉及到消费者剩余和生产者剩余，两者是完全不同的。 总需求曲线向右下方倾斜有三点主要原因： 财富效应：物价水平下降提高了货币价值，鼓励消费者更多地支出。 利率效应：消费者购买物品需要的货币减少，人们会储蓄多余货币或购买债券(可贷资金供给变多)而导致利率下降，低利率促使生产者更多投资购买设备。 汇率效应：国内利率下降导致资本净流出增多，汇率下降，外国物品相对于本国物品就变贵了，这增加了本国的净出口。 对于总供给曲线，其实有短期和长期之分，长期总供给曲线应该是垂直的即不受物价水平影响，因为一国的生产率等真实变量不受物价水平(名义变量)的影响。而短期总供给曲线向右上方倾斜有以下三种理论解释： 黏性工资理论：由于劳动合同规定了工资量，因此工资不能马上随物价水平调整，当物价水平下降时，无法降低工资的公司只能降低产量来减少成本。 黏性价格理论：当物价发生变化时，有些企业并不能马上改变商品价格，更改价格可能要付出调整广告、印刷和分发新目录等成本，由于滞后的高价格会降低销量，企业就有减产的激励。 错觉理论：生产者没有意识到所有市场的价格都在变动，单以为自己所处的市场价格下降了，因此发生了减产。 在前一节中我们说货币注入导致货币贬值从而使物价升高，如果用这节的总供给需求模型来描述，则是货币注入降低了利率鼓励了更多投资，使总需求增加，从而让均衡物价水平升高。 由于利率效应的存在，中央银行可以通过货币政策来调节总需求从而刺激经济或者抑制过热的经济。 7. 通货膨胀与失业的短期权衡取舍 经济学家菲利普斯在研究英国经济时发现了失业率与通货膨胀之间的负相关关系，即低失业年份有高通货膨胀而高失业年份伴随着低通货膨胀。 从总供给需求模型的角度来看，短期时间内总需求的增加会导致物价上升、产量增多，产量的增多则意味着更多的就业，因此低失业率是合乎情理的。 除了从宏观角度解释，这种负相关性的出现还与人们对通货膨胀的预期有关，其关系可以概括成下式： 失业率=自然失业率−α(实际通货膨胀率−预期通货膨胀率)失业率 = 自然失业率-\\alpha(实际通货膨胀率 - 预期通货膨胀率) 失业率=自然失业率−α(实际通货膨胀率−预期通货膨胀率) 其中自然失业率是一个真实变量，是一个经济中不可避免的失业水平。α\\alphaα衡量人们对未预测到的通货膨胀的反应程度。当实际的通货膨胀率高于人们预期的通货膨胀率时，工资水平是根据低通货膨胀水平制定的，在高通货膨胀水平下是偏低的，而价格的上升却高于预期，企业就有增产的激励。 但是人们的预期是会调整的，当人们的预期跟上实际后工资会调整回正常水平，而且我们知道，长期供给曲线是垂直不受物价水平影响的，长期的产量也会回归到由生产率决定的产量下。因此，通过货币注入导致通货膨胀来降低失业的方法只在短期中有效，长期中的菲利普斯曲线也是垂直的，失业率会最终回归到自然失业率。","tags":["读书"],"categories":["笔记"]},{"title":"《经济学原理》微观经济学分册总结","path":"/2023/08/04/《经济学原理》微观经济学分册总结/","content":"相比于宏观经济学，微观经济学研究的是个体或者单个市场的经济行为和决策。微观经济学分册主要通过分析和比较的方式来推导和解释市场现象和制度，所以该书首先介绍了经济学的十大原理，然后引出重要的分析工具——供给与需求曲线，然后再具体到赋税、国际贸易、垄断等市场行为对供给需求曲线的影响，分析这些行为如何达到最终的目的——有利可图。 1. 经济学十大原理 经济学研究人们如何做出决策、人们如何相互影响、整体经济如何运行，而在每个方面都有若干条具有概括意义的原理。 人们如何做出决策： 人们面临权衡取舍。这是人们需要做出决策的前提，大多数情况下不可能做到既要…又要…，比如有时常常需要在效率和公平之间做选择。 某种东西的成本是为了得到它所放弃的东西，即机会成本。机会成本不单单包括放弃掉的经济收益，还包括时间等不以金钱为单位的东西。比如读研究生的机会成本就包括放弃掉的就业的收入和相应的时间花费。 理性人考虑边际量。边际变动是对当前计划的微小增量调整，理性人通过一步步的边际变动逐渐达到利益最大化。 人们会对激励做出反应。政府决策可以通过影响激励来改变人们的行为。 人们如何相互影响： 贸易可以让每个人状况变得更好。各国可以专注生产自己最擅长生产的产品，然后通过贸易来交换其他种类的产品，专业化提高了整体的效率。 市场是组织经济活动的好方法。市场上每个人都只关心自己利益的最大化，但在大多数情况下却会实现整个社会福利的最大化。 政府有时可以改善市场结果。市场在某些情况下也会失灵，需要政府介入。 整体经济如何运行： 一国的生活水平取决于它生产物品与服务的能力。一国生产率的增长率决定了它的平均收入的增长率。 当政府发行了过多货币时，物价上升。 社会面临通货膨胀与失业之间的短期权衡取舍。货币量增加刺激社会整体支出水平从而增大需求，激励企业雇佣更多工人生产产品。 微观经济学展开描述了关于人们如何进行决策和人们如何相互影响的经济学原理，而宏观经济学则详细描述整体经济如何运行的经济学原理。 2. 需求曲线与供给曲线 需求曲线用来表示一种物品的价格和需求量之间的关系，而供给曲线则用来表示一种物品的价格和供给量之间的关系。一般情况下，需求曲线向右下方倾斜，供给曲线向右上方倾斜。 需求曲线向右下方倾斜的现实意义是当一样物品的价格上升时，其需求量下降，消费者会选择更多购买该物品的替代品。供给曲线向右上方倾斜的现实意义是当一样物品的价格上升时，其供给量也上升。 曲线的移动：当需求(供给)增加时，需求(供给)曲线向右移动，意味着在同样的价格下，需求(供给)量增大，需求(供给)减少时则向左移动。 供给和需求曲线的交点被称为均衡点，在均衡点上方的情况被称为过剩，均衡点下方的情况称为短缺。在供给过剩的情况下，卖者无法卖出全部产品，因此需要降低价格，价格的降低使供给减少需求增大，最终达到均衡点；在供给短缺的情况下，有过量的消费者购买产品，卖者上升价格不会减少销量反而会因为供给变多增加销量，价格上升使供给增加需求减少从而达到均衡点。 3. 从福利经济学看供给需求曲线 将买者愿意为某种物品支付的最高量称为支付意愿，买者会拒绝购买价格高于自己支付意愿的商品。 假设对某商品A的买者的支付意愿进行统计，以10元为间隔，最终统计出支付意愿在100元的有10人，支付意愿在90元的有20人，支付意愿在80元的有15人；则当该商品价格低于80元时有10+20+15=45人愿意购买，价格高于80低于90时有10+20=30人愿意购买，价格高于90低于100时只有10人愿意购买，而价格高于100时无人愿意购买。将该结果绘制成曲线如下： 价格每下降10元时，愿意支付的买者数量就会相应增加，如果我们将价格间隔从10元改成5元，则是每下降五元就会有新的买者加入市场，可以想象，当我们把这个价格间隔不断缩小，这条曲线就会越来越平滑，最终就变成了我们之前看到的需求曲线。需求曲线可以看成是消费者支付意愿的反映。 对于生产者来说也是一样，生产者生产一件商品需要成本，这个成本既包括生产时的直接经济支出，还包括时间等其他机会成本，因此对于生产者来说也存在一个销售意愿，只有当价格高于销售意愿时生产者才愿意售出产品。随着价格的升高，高出一些生产者的销售意愿之后，就会有新的生产者进入市场，因此供给曲线是一条向右上方倾斜的曲线。 对消费者来说，若商品价格低于其支付意愿，则支付意愿和商品价格之间的差值是消费者从参与市场中获得的利益，将其称为消费者剩余。同理，商品价格和销售意愿之间的差值被称为生产者剩余。 当价格下降时，原来就参与市场的买者的消费者剩余会增加，新加入市场的消费者也有相应的消费者剩余，市场上的总消费者剩余增加；但是对卖者来说，价格下降会导致有生产者退出市场，仍然参与市场的生产者的生产者剩余也会下降，市场上的总生产者剩余减少。价格上升时则反之。 市场上的总剩余等于消费者剩余加上生产者剩余，当市场位于均衡点时总剩余最大。因为当供给过剩时，供给量大于需求量，生产者的销售量等于需求量，有一部分产品由于未被售出而没有给生产者带来剩余；当供给短缺时，供给量小于需求量，销售量就等于供给量，有一部分消费者没能买到产品因而也没有消费者剩余。 4. 国际贸易对供给需求的影响 当一个国内市场还没有参加国际贸易时，存在国内价格和世界价格(均为均衡状态)不同的情况。当国内市场参加国际贸易后，国内价格就会自发调节到世界价格。如果国内价格低于世界价格，生产者就有进行出口的激励；如果国内价格高于世界价格，消费者就有购买进口品的激励。 如果一个国家成为出口国，国内价格就会上升到世界价格，根据前面的结论，国内的消费者剩余就要减少，但是由于参加了国际贸易，生产者的销售量已经不等于国内的需求量，由于价格上升而多生产的产品可以出口到他国，因而生产者剩余会增加。 如果一个国家成为进口国，国内价格就会下降到世界价格，国内的生产者剩余就要减少，但是由于参加了国际贸易，消费者可以通过购买进口品来弥补国内的供给短缺，因而消费者剩余会增加。 变成出口国时，生产者状况变好但是消费者状况变坏了；变成进口国时，消费者状况变好但是生产者状况变坏了。但无论变成出口国还是进口国，国内的总剩余都因为参加了国际贸易而变得更多了，赢家的收益超过了输家的损失。 5. 税收对供给需求的影响 如果对卖者征税，要求卖者每售出一件物品就向政府支付一定量的税收，由于征税导致卖者销售成本升高，卖者如果想维持同样的供给量，必定需要提高销售价格，提高的幅度等于税收量以此抵消税收的影响，因此供给曲线向上移动，导致新的均衡点向左移动，税收缩小了市场规模。 如果对买者征税，买者每购买一件商品都要向政府支付一定税额，那么相当于提高了商品的价格，那么当商品价格不变的时候，买者购买量减少，需求曲线向左移动(也可以说向下移动，移动幅度等于税收量，即市场价格需要下降才能维持同样的购买量)。 无论对卖者还是买者征税，买者支付的价格都上升，卖者得到的价格都下降，需要二者共同分担税收负担，区别仅在于谁对税收直接付钱。无论对何者征税，买者实际支付的价格和卖者实际得到的价格的差值都等于税收量，征税往供给需求曲线中打入了一个三角形的楔子。 要研究卖者和买者各分担了多少税收负担，就需要研究供给弹性和需求弹性。 供给量和需求量会随价格变动而发生变化，我们用价格弹性来衡量需求量或者供给量对价格变动的反应程度。当价格变动时，如果供给量或者需求量变化很大，就称其为富有弹性的，如果变化很小，则称其为缺乏弹性的。反映在图形上则是约平坦的曲线价格弹性越大，约陡峭的曲线价格弹性越小。 当需求缺乏弹性而供给富有弹性时，税收更多由消费者承担，打入一个税收楔子后卖者得到的价格并没有下降太多，而买者支付的价格上升得更多。对需求富有弹性而供给缺乏弹性的市场征税时情况则反之。(图中没有画出新的供给或需求曲线，因为无论对谁征税，其结果是一样的) 因此税收负担更多落到弹性较小的一方身上。弹性衡量的是价格变得不利时买者卖者离开市场的意愿，弹性小意味着没有其他的替代品，不太愿意离开市场，从而必须承担更多的税收负担。 再研究赋税之后的生产者剩余、消费者剩余和政府收入。政府的收入等于税收量乘以新均衡点下的商品购买量，而生产者剩余是卖者得到价格和供给曲线所围面积，消费者剩余是买者得到价格和需求曲线所围面积。 可以看到有税收时的总剩余相比无税收的总剩余减少了一块面积，这一块减少的总剩余称为无谓损失。由于无谓损失的存在，买者和卖者减少的剩余要大于政府获得的收入。由于税收导致有人退出市场，政府自然无法征收到退出市场的人的税收，因此就存在无谓损失。供给和需求的弹性越小，无谓损失也越小，因为退出市场的人少。 6. 成本曲线 企业想要达到利润最大化，那么不仅仅要关注市场价格，还要关注自己的生产成本和产量的问题。 企业想要增大产量就要增大投入提高成本，当产量比较低的时候，要提升一定的产量只需投入较少的成本，但是当产量越来越高时，提升单位产量所需要的投入成本就会变得越来越高，比如企业起步时每雇佣多一个工人就能更充分地利用生产设备从而提高产量，但是随着工人数量增加，再增加工人可能使工人在拥挤条件下工作反而互相妨碍，导致每个增加的工人贡献的产量越来越小，这就是边际产量递减。 边际产量是增加一单位投入引起的产量增加，边际产量递减就是一种投入的边际产量随着投入量增加而减少的特征。画出产量与总成本的关系如下： 除了总成本，还有其他用来衡量成本的指标： 固定成本：不随产量变动而变动的成本，即使企业根本不生产也要发生的成本 可变成本：随产量变动而变动的成本。 边际成本：额外一单位产量所引起的总成本的增加。 企业通常还想知道某产量下一单位产品的成本有多少，这样就有了平均总成本(ATC)、平均固定成本(AFC)、平均可变成本(AVC)，企业常常考察这些平均成本、边际成本(MC)与产量之间的关系来进行短期决策和长期决策。 对于边际成本，由于产量越高，提升一产量所要的投入越多，因此边际成本随产量增加而增加。 对于平均固定成本，由于固定成本不随产量变化而变化，因此平均固定成本随产量增加而减小，呈反比关系。 给定产量x，将0到x所有对应的边际成本求和即可得到产量x下的可变成本，换句话说，可变成本的一阶差分等于边际成本。由于边际成本递增，那么平均可变成本也一定是递增的，且平均可变成本低于边际成本。 产量 0 1 2 3 边际成本 0.2 0.4 0.5 0.7 可变成本 0 0.2 0.6 1.1 对于平均总成本，其为平均固定成本和平均可变成本之和，平均固定成本随产量增加而减小，平均可变成本随产量增加而增加。在产量较低时，平均固定成本下降速度较快，占主导地位，因此平均总成本下降；产量较高时，平均固定成本下降速度放缓(反比曲线)，平均可变成本增加占主导地位，平均总成本升高。 各平均成本和产量关系如下： 平均总成本曲线的底端时的产量称为有效规模，此时平均总成本最小。平均总成本曲线的低点就是平均总成本曲线和边际成本曲线的交点，边际成本大于平均总成本时平均总成本才开始增加，相当于只有当下一门考试成绩大于目前的平均成绩时，平均成绩才会上升。 实际上，边际成本并不是一直递增的，企业并不是在雇佣了第一个工人后边际产量就开始递减，人少的时候多雇佣一些人可以更好的分工，从而边际产量增加，雇佣人数到了一定程度才会发生边际产量递减。因此边际成本有一段先下降的过程，典型的成本曲线如下： 7. 竞争市场 竞争市场是有许多买者和卖者的市场，每个买者卖者对市场价格的影响可以忽略不计，因此他们都是价格接受者。 企业每多生产一单位产品获得的利润称为边际利润，边际利润应该等于边际收入减去边际成本，在竞争市场下，企业无论增产还是减产都无法影响市场价格，因此此时边际收入等于市场价格。当边际利润大于0时，企业增产就能增加收入，当边际利润小于0时，企业需要减产来达到最大利润。因此，当企业的边际成本等于市场价格时，企业的利润最大化。 当我们将这种关系画成曲线我们就发现边际成本曲线和供给曲线是非常相似的，在供给曲线中，给定一个市场价格就能得到市场上的供给量，在边际成本曲线中也是一样，当企业知道了市场价格之后，同样也可以确定自己利润最大化的供给量，似乎可以认为边际成本曲线就是一个企业的供给曲线。 但实际上边际成本曲线与供给曲线有一点很大的不同，可以想象，当市场价格不断降低时，企业的利润会从最大值一直减小到0，市场价格再降低，企业就应该停止生产了，而不是等到价格为0才停止生产。所以边际成本曲线只是一部分上等于企业的供给曲线。 我们假设，在短期决策中企业不考虑退出市场，在长期决策中企业可以自由进出市场。如果企业决定短期停止营业，则企业不再需要付出可变成本，但是仍然需要支出固定成本。因此当从事生产时的收益＜可变成本时，企业暂时停止营业，不等式两边同时除以产量，则变为价格＜平均可变成本时，企业暂时停止营业。结合前面边际成本曲线与供给曲线相似的结论，可知企业的短期供给曲线是边际成本曲线位于平均可变成本曲线之上的部分。 在长期决策中，企业会考虑退出市场，此时企业不再需要支出可变成本和固定成本，即收益＜总成本时企业退出市场，不等式两边同时除以产量，得到价格＜平均总成本时企业退出市场。因此，企业的长期供给曲线是位于平均总成本之上的部分。 需要注意的是，边际成本曲线与平均可变成本相交时的价格是低于与平均总成本相交时的价格的。在短期中，如果价格高于平均可变成本但是低于平均总成本，此时收益是无法补偿总成本的，是存在亏损的，但是足以补偿投入的可变成本，且我们假设企业短期内不考虑退出市场，因此即使此时存在亏损也不至于退出市场，可以等待价格回升。但是如果价格长期低于平均总成本，企业只能退出市场。 上面考察的都是单个企业的供给曲线，接下来需要考察整个市场上的供给曲线。 对于短期供给曲线，由于假设不存在企业进入退出市场，因此只需要把市场上所有企业的短期供给曲线加和即可得到市场的短期供给曲线，最后得出的图形和我们之前画的供给曲线一致。 对于长期供给曲线，由于企业可以自由进出，当市场上现有企业盈利时就会有企业加入市场，导致供给增加、价格减少、利润减少；当市场上现有企业亏损时就会有企业退出市场，导致供给减少、价格升高、利润增多。当达到市场均衡时，仍处在市场中的企业经济利润为零。由于利润为零，因此价格只能等于平均总成本，因此长期供给曲线是一条水平线。长期供给曲线上的点是处在市场均衡状态下的，该价格下的供给量还需要取决于需求量即均衡点的位置。 当市场达到均衡时，假如市场需求增加，则会使得价格上升，此时在市场中的企业短期内获得利润，短期均衡点位于长期供应曲线上方；在长期范围内则会有新的企业加入市场，导致供给增加、价格下降，导致市场内的企业经济利润重新为零，新的均衡点重新落到长期供给曲线上。 8. 垄断 在竞争市场中，企业是价格接受者，在垄断市场中，垄断企业(市场上的唯一卖者)是价格决定者，两者的目标同样是利润最大化，但是却有很不同的市场结果。 由于垄断市场上只有垄断企业一个卖者，因此市场的需求曲线就是企业的需求曲线，在某一定价下的需求量就是企业的供给量(由于没有竞争，垄断者可以市场需求多少产品就生产多少产品)。对于竞争企业来说，提高产量并不会影响市场价格，因此其边际收益等于市场价格；对于垄断企业来说，它按照需求曲线进行供给，因此增加产量时价格下降，其边际收益小于市场价格，平均收益等于市场价格。当价格下降过多时，即使销售量增加了，总收益也会减少，导致边际收益小于零。 无论是竞争市场还是垄断市场，边际利润还是等于边际收益减去边际成本，只要边际利润还大于0，企业增产就能提升利润。因此垄断者的利润最大化产量是由边际收益曲线与边际成本曲线的交点决定的。但是由于需求曲线在边际收益曲线上方，因此此时的价格是高于边际成本的。(对于竞争企业来说，利润最大化时边际成本等于价格) 垄断使得企业可以收取更高的价格从而获得比同产量下竞争企业更多的利润，这对企业所有者是合意的，但是对消费者来说是不合意的。我们可以通过分析总剩余来考察垄断对整个社会是否合意，对于竞争企业来说我们使用供给曲线来衡量其销售成本，但是对于垄断企业我们要用边际成本曲线来衡量其销售成本。 需求曲线和边际成本曲线相交时的产量是效率产量，此时总剩余最大，但是垄断企业的产量低于效率产量，垄断价格高于边际成本导致有消费者拒绝进入市场，出现无谓损失，因此垄断是无效率的。","tags":["读书"],"categories":["笔记"]},{"title":"使用词向量和余弦相似度进行文本查重","path":"/2023/06/24/使用词向量和余弦相似度进行文本查重/","content":"Word2vec是一种用于自然语言处理的算法，它可以将文本中的单词表示为高维向量，这些向量可以被用来计算单词之间的相似度。而余弦相似度是一种用于计算向量之间相似度的度量方法，本文使用word2vec和余弦相似度结合的方法来进行文本的查重。 1. Word2Vec 1.1 基本思想 word2vec是一种基于神经网络的自然语言处理算法，通过学习大量文本语料库中单词的上下文信息，将单词转换为固定长度的高维向量。 word⟹[ 0.881, 0.126, 0.753, 0.294, … , 0.745 ]⏞embedding lengthword \\Longrightarrow \\overbrace{[ \\,\\, 0.881,\\,0.126,\\,0.753,\\,0.294,\\,…\\,,\\,0.745\\,\\,]}^{embedding\\,\\,length} word⟹[0.881,0.126,0.753,0.294,…,0.745]​embeddinglength​ word2vec的基本思想是根据单词的上下文来学习单词的语义，这就意味着那些经常出现在相同上下文中的单词，映射到向量空间后其欧几里得距离会比较相近。比如给神经网络输入两个句子：He loved the big old house. He loved the large old house. 由于big和large出现在了相似的上下文中，因此神经网络会认为它们具有相近的语义，然后将它们映射到同一片向量空间中，当上下文相似的句子出现得越多，两个单词在向量空间中就会越接近。 图片仅作示例，实际上的词向量维度要远大于3 1.2 网络结构与两种模型 word2vec的网络结构相当简单，其仅仅包含输入层、一个隐藏层和输出层，我们需要的词向量可以从输入层到隐藏层的权重矩阵中提取，或者从隐藏层到输出层的权重矩阵中提取，具体训练过程下文描述。 word2vec有两种不同的模型，一种是Continuous Bag-of-Words Model(CBOW)，一种是Contiguous Skip-gram Model(Skip-gram)。 两种模型的区别主要在于输入和输出的形式。对于CBOW模型，其任务是根据上下文来预测当前出现的单词，相当于选词填空。首先从句子中选择一个词作为目标值，然后将指定长度的上下文作为输入来训练神经网络。比如： 而Skip-gram模型则相反，其要求输入一个词，然后根据这个词来预测指定长度的上下文。 1.3 训练过程 word2vec的输入向量和输出向量都用one-hot的形式给出，所谓one-hot向量，即向量中仅有一个元素为1，而剩余元素皆为0的向量。首先我们要确定我们字典(所有出现在训练集和测试集中的词的集合)的大小，然后给每一个词一个唯一的索引。则所有词的one-hot向量长度固定为字典大小，一个词的one-hot向量即索引处为1，其余处为0的向量。比如： dict.length==5dict[2]=′big′big的one−hot向量: [0,0,1,0,0]dict.length == 5\\\\ dict[2] = big\\\\ big的one-hot向量:\\,[0,0,1,0,0] dict.length==5dict[2]=′big′big的one−hot向量:[0,0,1,0,0] 本项目使用的是CBOW模型，下面以CBOW模型为例来进行说明。 首先需要遍历整个训练集，找出所有出现的不同的字，并且给每一个字一个唯一的id。 然后在训练集中取出一段文本，使用一个滑动窗口(以大小为3的窗口为例)来取得一个个词袋，取中间的字作为目标值，剩下的字作为输入值。 对于每一个词袋，将其拆成输入字与目标字的一对，输入字和目标字都转换成one-hot向量，故训练时是一个一个字地输入到网络当中。 处理完训练集中所有文本后得到所有的输入-目标对，就可以进行训练，设字典大小为vocab_size，要获得的词向量的维数为embedding_size，则输入张量的形状为[batch_size, vocab_size]，输出张量的形状为[batch_size, vocab_size]，可以得到网络结构如下(pytorch)： class VocabEmbedding(nn.Module): def __init__(self): super(VocabEmbedding, self).__init__() self.W = nn.Linear(vocab_size, embedding_size, bias=False) self.WT = nn.Linear(embedding_size, vocab_size, bias=False) def forward(self, X): hidden_layer = self.W(X) output_layer = self.WT(hidden_layer) return output_layer 权重矩阵的形状为[vocab_size, embedding_size]，当一个one-hot向量与其相乘时，因为one-hot向量中只有一个元素为1，所以计算结果实际上只与权重矩阵中的一行有关，计算输出值和目标值的误差后进行反向传播，也只有权重矩阵中的一行会被更新，因为其他行对输出值没有贡献。以vocab_size=5, embedding_size=7为例： 训练完毕后，我们只需在权重矩阵中按词的id取出相应的行向量即可作为每个词的向量。 2. 余弦相似度 余弦相似度使用两个向量之间的夹角来衡量两个向量之间的相似程度，其计算公式如下： similarity=cosθ=x⃗⋅y⃗∣∣x∣∣ ⋅ ∣∣y∣∣similarity = cos\\theta = \\frac{\\vec{x}·\\vec{y}}{||x||\\,·\\,||y||} similarity=cosθ=∣∣x∣∣⋅∣∣y∣∣x⋅y​​ 欧氏距离更加注重两个向量在数值上的差异，余弦相似度则注重两个向量在方向上的差异。 3. 文本查重 3.1 句子的向量表示 通过word2vec，我们已经得到了每个字的向量表示，要得到每个句子的向量表示，我选择了直接将一个句子中所有字的向量简单加和，这样每个句子的向量都具有相同的长度，更加方便处理。 但是，这种做法丢失了字与字之间的时序信息，导致具有相同字但顺序不同的句子也被检测为相似，比如“国立武汉大学”和“学大汉武立国”在这种方法下会被认定为是完全相同的句子。 3.2 检测 将一篇文章分句后，再获得每个句子的向量，将这些向量组合成一个矩阵，这个矩阵就代表了这篇文章的内容。在组合成矩阵之前先将句子向量进行单位化，这样的话将两个文章的矩阵直接相乘就可以计算处两篇文章所有句子两两之间的余弦相似度。两篇文章的矩阵形状分别为[num_sentences, embedding_size]，[embedding_size, num_sentence]。 4. 仓库地址 eumendies/plagiarism_detection: plagiarism detection with word2vec and cosine similarity (github.com)","tags":["NLP"],"categories":["deep-learning"]},{"title":"简","path":"/2023/04/07/简/","content":"最近常常因物烦心，有所感想。 舍不得的东西总有一天会拖累自己，当你觉得“以后可能还用得上”的时候，那这个东西大概率已经没用了 当对一件物质的顾虑(如何处置、是否需要等一切耗费心神的想法)超出其实用性(精神层面与物质层面)时，则应该舍弃 若当前所有物品已经满足所需的必要功能，构成全功能集，继续添置物品为冗余 冗余增多，心理负担增多，精神越不自由，简单即减少冗余 各人需求的功能不同，全功能集大小也不同，故简单是相对量，而不是根据物品的多少而确定的绝对量，不是为简而简(他们极简主义者睡觉都不盖被子的吗？) 一样物品应该专精其主要功能，只有当增加的实用性超过增加的冗余时才整合其他非必要功能 物品能达到“精”才能进而“少”，物品不能满足需求的可能性太大才导致多留备用 我害怕这些会成为我不做某些事情的借口。","tags":["随笔"],"categories":["随笔"]},{"title":"verilog实现RISC-V流水线处理器","path":"/2023/03/27/verilog实现RISC-V流水线处理器/","content":"使用verilog语言描述的RISC-V流水线处理器，处理了数据冒险和控制冒险，github地址：流水线 RISC-V流水线数据通路 参考教材Computer Organization and Design: The Hardware/Software Interface，将整个数据通路分为五级流水线，分别为取指阶段(Instruction Fetch, IF)，解析指令阶段(Instruction Decode, ID)，执行阶段(EX)，访存阶段(MEM)，写回阶段(WB)，各模块也基本按照教材设计。 接下来介绍各大模块的实现。 （作者水平有限，代码和实现中可能仍存在不少纰漏和错误，欢迎读者指出） PC寄存器和PC加法器 PC寄存器接收一个新的PC值，如果输入的PCwrite信号为高，则在时钟上升沿将输入的新PC值保存，否则不改变原来的PC值；当发生数据冒险和控制冒险时，需要通过PCwrite信号来阻止PC值的更新。 module PC_reg(\tinput clk, rstn, PCwrite, input[31:0] nextPC, output reg[31:0] nowPC\t); PC加法器用于计算新的PC值并将该值输入到PC寄存器，当没有跳转指令执行时，PC加法器简单地将目前的PC值加4并写回PC寄存器，当跳转发生时，PC加法器计算出目标地址后再写回PC寄存器。五级流水线的数据通路需要在MEM阶段计算出分支目标地址，所以其取EX/MEM流水线寄存器中保存的PC值和立即数字段来计算新的PC值。 module PC_adder( input[31:0] nowPC, input[31:0] EX_MEM_PC, EX_MEM_imm, input Branch, jump, output[31:0] newPC\t); 流水线寄存器 由于一个数据通路中有多条指令在执行，所以需要流水线寄存器来保存指令相关的信息，如果不用流水线寄存器保存，则当下一条指令载入数据通路时，上一条指令的PC值、控制信号等就会丢失。 五级流水线需要四个流水线寄存器：IF/ID流水线寄存器、ID/EX流水线寄存器、EX/MEM流水线寄存器、MEM/WB流水线寄存器。当流水线寄存器中一个信息在某一阶段被使用了，那么在下一个流水线寄存器中就不再需要保存该值；随着指令在流水线中的流动，也会有新的信息需要保存到流水线寄存器中。 处理数据冒险和控制冒险时需要在流水线中插入空指令，当flush信号为高时，就把寄存器中全部信息清零。 module ID_EX_reg( input clk, rstn, flush input[31:0] instr, input[31:0] PC, imm, RD1, RD2, input regwrite, ALUSrc, MemRead, MemWrite, MemtoReg, Branch, input[3:0] ALUControl, output[31:0] ID_EX_instr, output[31:0] ID_EX_PC, ID_EX_imm, ID_EX_RD1, ID_EX_RD2, output ID_EX_regwrite, ID_EX_ALUSrc, ID_EX_MemRead, output ID_EX_MemWrite, ID_EX_MemtoReg, ID_EX_Branch, output[3:0] ID_EX_ALUControl\t); 控制器 控制器根据输入的指令来生成对应的控制信号，其首先比对指令中0到6位的操作码(opcode)来确定指令的类型并产生对应的信号。对于R型指令，需要根据funct3和funct7字段来确认ALU所要执行的操作，对于一些I型指令，需要根据funct3字段确定ALUControl。 module Controller( input[31:0] instr, output regwrite, ALUSrc, MemRead, MemWrite, MemtoReg, Branch output[3:0] ALUControl\t); 立即数生成 I型、S型、SB型、U型和UJ型指令都需要提取立即数字段，其中SB型和UJ型指令中的立即数表示的是半字地址，需要左移一位来转换成字节地址。 module ImmGen( input[31:0] instr, output[31:0] imm\t); 寄存器堆 寄存器堆中有32个64位的寄存器，写信号为高时，将输入的数据写入rd寄存器号对应的寄存器中；同时还需要从寄存器堆中读取rs1、rs2寄存器的值并输出。 其中写入数据WD、寄存器号rd和写使能信号是来自流水线最后一级，rs1和rs2则来自IF/ID流水线寄存器。 module RegFile(\tinput clk, rstn, RFWr, input[4:0] rs1, rs2, rd, input[31:0] WD, output[31:0] RD1, RD2\t); reg[31:0] rf[31:0]; ......endmodule 寄存器堆前递单元 由于寄存器堆是异步写（在always块执行完毕后才更新寄存器）、同步读的，而寄存器堆和ID/EX流水线寄存器都是在时钟上升沿进行更新，所以当ID阶段要读取的寄存器和WB阶段要写的寄存器相同时，会导致读取出来的值是更新前的值而不是WB阶段写回的新值。 如果采取在上升沿写入、下降沿读取寄存器堆的解决方案，ID/EX流水线寄存器由于是在上升沿更新，所以就无法取得读取出来的值。因此我采取了添加前递单元的解决方案，当要读取的寄存器等于要写入的寄存器时，就将WB阶段的写回值前递给ID/EX流水线寄存器。 module Forwarding_WB( input MEM_WB_regwrite, input[4:0] MEM_WB_rd, input[4:0] IF_ID_rs1, IF_ID_rs2, output reg forward_RD1, forward_RD2 ); Comparator 比较器用于确定各种跳转指令是否满足跳转条件，并输出信号指示是否跳转 module Comparator( input[31:0] instr, input[31:0] A, B, output jump\t); ALU ALU接受两个输入数据，然后根据ALUControl来决定要进行的计算并给出输出。 module( input signed[31:0] A, B, input[3:0] ALUControl, output signed[31:0] result\t); RAM 使用256个8位宽的寄存器来组成RAM，即有256个字节。当MemWrite信号为高时，将传入的数据从传入的内存地址开始一个字节一个字节地往后写，读取数据时也是从给定的内存地址开始逐字节读取。 同时还要根据指令的funct3字段来确定要写入的数据长度 module RAM(\tinput clk, rstn, input MemWrite, MemRead, input[2:0] instr_funct3, input[31:0] address, input[31:0] WD, output[31:0] ReadData\t); RAM前递单元 对于store指令来说，要存储到内存中的寄存器值可能依赖于前一条指令的结果，因此可能需要将MEM/WB流水线寄存器中的值前递一个阶段 module Forwarding_store( input EX_MEM_MemWrite, input[4:0] EX_MEM_rs2, input MEM_WB_regwrite, input[4:0] MEM_WB_rd, output reg forward_store ); RAM数据裁剪 对于lb、lh、lw三条指令，其各自要读取的数据长度分别为1个字节、2个字节、4个字节，而从RAM中读取数据时都是直接从地址开始读取4个字节，因此需要添加一个组合逻辑模块来裁剪多读取的数据。 除了要裁剪数据，还要区分lb、lh、lbu、lhu，决定是否需要进行符号扩展。 module Load_Clip( input[2:0] instr_funct3, input[31:0] t_mem_data, output[31:0] mem_data ); ALU前递单元 当一条指令的操作数依赖于前一条指令或前前一条指令的结果时会发生数据冒险，如果之前指令的写回结果在EX阶段就能得到，那么就可以通过前递来解决这种数据冒险。 当一条指令需要前一条指令的结果，可以将前一条指令的结果从EX/MEM寄存器中前递给将要进行ALU计算的指令。 当一条指令需要前前一条指令的结果，可以从MEM/WB寄存器中前递数据。 只有当EX/MEM寄存器不进行前递时才考虑MEM/WB寄存器是否进行前递，因为EX/MEM中的结果更新，写回后会覆盖MEM/WB中的结果。 module Forwarding(\tinput EX_MEM_regwrite, MEM_WB_regwrite, input[4:0] EX_MEM_rd, MEM_WB_rd, input[4:0] ID_EX_rs1, ID_EX_rs2, output[1:0] forwardA, forwardB\t); 冒险检测单元 load-use数据冒险不能单纯通过前递来解决，因为当指令需要使用内存中的数据时上一条指令还没有访存并得到数据，所以无法前递。 当检测到ID/EX寄存器中的指令需要读取内存，且其要写入的寄存器等于IF/ID寄存器中的指令要使用的寄存器，这时候就需要清空IF/ID寄存器，这样就插入了一条空指令，并且阻止PC寄存器更新，下一个时钟周期又会取出同一条指令加载进IF/ID寄存器中。如此，两条指令之间就间隔了一个阶段，访存的指令获得数据之后就可以将数据前递。 对于控制冒险，这里采取了最简单的延迟分支处理方式，向流水线中插入三条空指令，分支指令在Mem阶段决定好分支目标时再更新PC寄存器的值。 module Hazard_unit(\tinput ID_EX_MemRead, input[4:0] ID_EX_rd, IF_ID_rs1, IF_ID_rs2, input[6:0] IF_ID_opcode, ID_EX_opcode, EX_MEM_opcode, output PCwrite, IF_ID_write, IF_ID_flush, ID_EX_flush\t);","tags":["计算机组成原理","verilog"],"categories":["计算机基础"]},{"title":"xv6的虚拟存储","path":"/2023/02/16/xv6的虚拟存储/","content":"xv6是MIT为其操作系统课程开发的一个教学目的操作系统，本文将介绍xv6的虚拟存储。 1 基本配置 xv6中页(page)的大小是4096(2122^{12}212)字节，因此虚拟地址中需要12位来索引页中字节，此外还需要27位来索引页表中的物理页号，所以实际上xv6只使用了64位虚拟地址中的39位，保留了高25位。 一个页表表项（page table entry，后面简称pte）中包含了一个44位物理页号，与虚拟地址中的12位页内偏移合并即为56位的物理地址；除此之外还有10位权限位。 由于页表中常常含有大量的空映射，为了节约存储空间，xv6采用了三级页表，将虚拟地址中27位索引位分成3份各9位分别索引各级页表。如果采用单级页表，创建页表时需要在内存中留出2272^{27}227个pte的空间；而三级页表则是先创建一个含有292^{9}29个pte的页表，当添加映射时才需要创建后两级页表，实现了表项的稀疏存储。 2 虚拟地址的转换 2.1 在用户态下的虚拟地址转换 假设用户程序要解引用一个指针，这个指针当中存储的64位地址会被视为虚拟地址，内存管理单元(MMU)会到用户程序的页表中找到虚拟地址相应的表项，然后将虚拟地址转换成物理地址。satp寄存器中会存储当前程序页表的物理地址，MMU则会使用这个页表进行转换。 satp寄存器指向第二级页表，MMU使用虚拟页号高9位取得pte，该pte中的物理页号是第一级页表的物理地址，使用虚拟页号中9位索引第一级页表得到第二个pte，这个pte指向第零级页表，最后用低9位索引第零级页表得到最终的pte，将这个pte中的物理页号和虚拟地址中12位偏移量结合得最终的物理地址。 2.2 在内核态下的虚拟地址转换 内核地址空间与用户程序地址空间的不同点在于内核地址空间使用直接映射，即虚拟地址的值和物理地址的值相同。地址空间中除了trampoline和kstack以外都采用了直接映射，稍后会看到直接映射的好处。 在内核态下解引用一个指针，MMU仍然会查询内核页表来转换虚拟地址，只不过转换后得到的物理地址和原来的虚拟地址相同。 图中的free memory区域是内核用来分配给用户程序的内存，因此用户程序使用的物理内存地址不仅在用户页表中存在映射，在内核页表中也存在映射，但是两个映射的虚拟地址不同。 2.3 内核态下需要使用用户提供的虚拟地址 用户程序常常需要内核执行一些任务并返回结果，因此会通过系统调用传递一个虚拟地址，希望内核将执行结果存储到内存中的相应位置。 但是用户程序和内核使用的是完全不相同的两个页表，内核并不能直接解引用这个虚拟地址，但是： 内核保存了用户页表的物理地址 内核可以通过程序在这个页表中查询到虚拟地址对应的物理地址PA，walk()函数实现了该任务 取得这个物理地址PA后，可以直接对其解引用，MMU会将PA视为虚拟地址，然后在内核页表中查询对应的物理地址 由于内核页表直接映射的特点，最后得到的物理地址和PA完全相同 注意，2和3是两个不同的过程，2是在软件中进行的，模拟了MMU查询页表的过程，而3是在硬件中进行的过程。这个过程的伪代码如下： copyout(userVA):\tuserPA = walk(user_pgtbl, userVA) *userPA = result 比如用户程序传递虚拟地址0x0000020000给内核，内核通过walk函数得到相应的物理地址0x00800e0000，对其进行解引用并赋值为结果值，MMU在内核页表中查询“虚拟地址”0x00800e0000对应的物理地址，查询结果为0x00800e0000，则CPU将结果值存入0x00800e0000。","tags":["操作系统"],"categories":["计算机基础"]},{"title":"xv6系统调用过程","path":"/2023/01/02/xv6系统调用过程/","content":"xv6是MIT为其操作系统课程开发的一个教学目的操作系统，其系统调用的过程是如何进行的？本文将以xv6的RISC-V版本进行说明。 1 系统调用 用户运行的应用程序处于用户态，处于用户态的进程受到诸多限制，某些工作必须由内核代为完成，因此用户进程需要使用系统调用。 内核态下可以获得的特权： 读写控制寄存器，包括satp、stvec、sepc等 可以使用页表中用户不可用的表项 2 xv6系统调用的过程 下图是整个过程的流程图，先从整体上感受其过程，由于某些命名比较相似，请注意不要混淆其功能，比如uservec、usertrap、userret 2.1 系统调用函数原型及汇编代码生成 xv6使用名为usys.pl的perl脚本在用户空间中生成一个包括所有系统调用的汇编代码 sub entry my $name = shift;\tprint .global $name ;\tprint $name: ;\tprint li a7, SYS_$name ;\tprint ecall ;\tprint ret ; 在perl脚本中调用entry(exit);就能在usys.S中生成如下汇编代码 .global exitexit:\tli a7, SYS_exit\tecall\tret 而fork函数的函数原型声明在user.h头文件中，当用户程序调用exit(0);时，就会将参数0存入a0参数寄存器，然后使用jalr指令跳转到上述汇编代码中，然后将系统调用号(宏定义SYS_exit)放入a7寄存器中，再由ecall指令发起系统调用，此时会将当前程序计数器pc寄存器的值保存至sepc控制寄存器中。 2.2 用户空间和内核空间的切换 stvec控制寄存器中保存着uservec函数的地址，当用户使用ecall指令后，会跳转到uservec函数，这个函数位于trampoline.S的汇编代码（由于用户态跳转内核态、内核态跳转用户态都需要经过这里面的代码，故命名为蹦床trampoline）中，uservec作用如下： 保存32个通用寄存器的值到一片指定的内存区域，这个区域称为trapframe，而sscratch寄存器中保存着trapframe的地址 将sp栈寄存器的值设为内核栈的地址，开始使用内核的栈；将tp寄存器的值设为该进程所处cpu的id 从trapframe中取得内核页表的地址，并设置satp寄存器指向内核页表(内核页表的地址是在用户进程被创建时放入trapframe的)，开始使用内核的页表 从trapframe中取得usertrap函数的地址，跳转到usertrap中 每个用户进程都在特定的内存地址保存trampoline的代码和trapframe，用户进程页表如下： 2.3 内核完成系统调用并返回 跳转进入usertrap函数后，usertrap执行流程如下： 读取sepc寄存器的值并保存到trapframe中，返回到用户空间时程序会从该值指向的指令处重新开始执行，目前sepc寄存器的值指向之前的ecall指令 读取scause寄存器的值，scause寄存器的值表明了用户陷入(trap)内核的原因，如果值为8则代表要进行系统调用 将trapframe中保存的sepc值+4，表明返回用户空间后从ecall的下一条指令开始执行 调用syscall函数 syscall函数执行流程如下： 从trapframe中取出a7寄存器的值，这个值是系统调用号，表明了要进行哪一个系统调用(是在usys.S中发起系统调用时放入a7寄存器的) 执行相应的系统调用函数，该函数会从trapframe中取出函数所需的参数并执行功能(所有参数寄存器都被保存在trapframe中) 将返回值保存到trapframe中，返回到用户空间时即可取得该返回值 相应的系统调用完成后，将会执行usertrapret函数，其执行流程如下： 在trapframe中保存内核页表的地址、内核栈的地址、cpu的id 将trapframe中保存的sepc值写入sepc寄存器中，由于在内核中也可能发生trap，会导致sepc寄存器的值被修改，所以现在要再次写一遍sepc寄存器的值 跳转到trampoline中的userret trapframe的初始化每个用户进程在内核中被首次创建出来后都会调用一遍usertrapret函数返回用户空间，从而其trapframe中会保存有内核页表地址等信息，使其可以完成系统调用。 trampoline中的userret执行流程如下： 将trapframe中保存的通用寄存器值恢复到32个寄存器中，因为trapframe中a0寄存器的值已经在syscall中被修改成了系统调用函数的返回值，因此恢复过程完毕后a0寄存器中就保存着返回值供用户程序使用 将satp寄存器设置为指向用户的页表(用户页表地址是userret函数的参数之一) 使用sret指令返回用户空间继续执行指令，会将pc寄存器的值设置为sepc寄存器的值，而sepc寄存器的值在usertrap中被保存，在usertrapret中被恢复 3 总结 再次放出流程图 3.1 相关的控制寄存器 sepc：发生trap时将pc寄存器的值保存；sret指令会跳转到sepc寄存器所指的指令 stvec：执行ecall指令后会跳转到的位置；在用户空间中指向trampoline中的uservec sccratch：保存着trapframe的内存地址 scause：其值说明了发生trap的原因，8代表系统调用 3.2 trapframe和trampoline trapframe：用于保存通用寄存器的值和系统调用的返回值，还保存了内核页表、内核栈的地址 trampoline：有用于从用户态跳转到内核态、从内核态跳转到用户态的两个函数uservec和userret 3.3 相关的函数 uservec：保存通用寄存器的值、切换页表和栈 usertrap：改写trapframe中的sepc值，调用syscall syscall：执行相应的系统调用函数，将返回值放入trapframe usertrapret：恢复sepc寄存器的值，将内核页表、内核栈的地址写入trapframe userret：恢复通用寄存器的值，切换页表和栈","tags":["操作系统"],"categories":["计算机基础"]},{"title":"详解RISC-V函数调用","path":"/2022/11/09/详解RISC-V函数调用/","content":"RISC-V函数调用时应该遵守什么规定，在函数调用的过程中发生了什么变化？ 1. 符号约定 本文使用32位RISC-V作为示例 寄存器： ra：返回地址(Return address)寄存器，即x1寄存器 sp：栈指针寄存器，即x2寄存器 t0-t2, t3-t6：临时寄存器，即x5-x7, x28-x31寄存器 s0-s1, s2-s11：保存寄存器，即x8-x9, x18-x27寄存器 a0-a7：函数参数寄存器，即x10-x17寄存器 2. jal和jalr 在函数调用中我们需要用到两条关键的指令：jal和jalr。我们通过jal发起函数调用，通过jalr来从函数中返回。 jump and link，jal rd, Label：UJ型指令，指令格式： immediate[20,10:1,11,19:12]rdopcode 使用jal指令时需要指定一个寄存器rd和一个标签Label（会在汇编阶段被翻译成立即数immediate），该指令会将PC+4保存到rd寄存器中，并且跳转到标签位置(PC + immediate)继续执行指令 jump and link register，jalr rd, imm(r1)：I型指令，指令格式： immediate[11:0]rs1funct3rdopcode 使用jalr指令时会将PC+4保存到rd寄存器中，并且跳转到地址rs1 + immediate继续执行指令 因为在汇编中我们用标签来表示后续的指令属于同一个函数，所以我们用可以跳转到标签的jal指令来发起函数调用。又因为jal将函数执行完毕后要继续执行的下一条指令的地址保存到了ra寄存器中，所以我们用可以跳转到寄存器中地址的jalr指令来从函数中返回。 使用jal和jalr的组合来调用函数： # 指令 # 指令的地址addi s0, x0, 1 # 0x0000 0000jal ra, funct1 # 0x0000 0004addi s1, x0, 2 # 0x0000 0008jal ra, exit # 0x0000 000cfunct1: sub t0, t1, t2\t# 0x0000 0010\tjalr x0, 0(ra)\t# 0x0000 0014\tfunct2: addi x0, x0, 0\t# 0x0000 0018\tjalr x0, 0(ra)\t# 0x0000 001cexit:\t# 使用系统调用退出程序，在此不展示细节 对jalr x0, 0(ra)的解释：在该条指令中我们使用x0寄存器作为rd寄存器来写入，由于x0寄存器的值始终为0，所以相当于丢弃掉了pc+4的地址，丢弃该地址是因为我们不需要使用该地址，我们从函数中跳转回来，return语句之后的指令地址是没有意义的。 同样，我们可以使用jal x0, Label来进行跳转，由于没有将pc+4保存，因此后续没办法再使用jalr指令跳转回来，相当于使用了一条C语言中的goto语句。 jal指令和jalr指令的错误用法：不要混淆两者进而写出jal x0, ra或者jalr x0, Label这样的指令！给jal指令提供跳转地址必须使用标签而不能通过寄存器，即使寄存器中存有一条指令的地址。而jalr是I型指令，意味着你必须提供两个寄存器和一个立即数，其中第二个寄存器中应该保存一条指令的地址。 3. 函数调用的过程 总结上述内容，可以将函数调用的过程概述如下： 将参数放到函数可以访问到的位置（参数寄存器a0-a7） 将控制转交给函数（使用jal指令） 函数获取任何其需要的存储资源 函数执行其功能 函数将返回值放在调用者(Caller)可以访问的位置（寄存器a0, a1） 将控制交还给调用者(使用jalr指令) 4. 函数调用约定(Calling convention) 在函数调用前，某些寄存器中可能存储了重要的数值，这些数值可能在我们完成函数调用后仍需使用，所以我们希望这些值在函数调用后仍然保持不变，因此我们需要遵守一些约定来达到这个目的。 最简单的方法是，当我们调用函数时，函数先将32个寄存器中的值全部保存到内存中，完成函数功能后再将内存中的值恢复到32个寄存器中。 然而这个方法不仅麻烦而且浪费，因此为了减少寄存器的换出，RISC-V将19个寄存器分成了两组：临时寄存器和保存寄存器，并遵守以下约定来维持保存寄存器在函数调用前后的不变性： 对于被调用者，其必须保存并恢复保存寄存器的内容，而不用保存临时寄存器中的内容；对于调用者，除了保存寄存器以外的寄存器的值如果在函数调用完成后还要使用，那么要由调用者自行保存（比如ra寄存器）。调用结束后，栈指针、保存寄存器的值和函数调用之前的值一样 实现了这个约定的函数可以分成三个部分： “序言”部分：如果该函数的主体需要使用到某个保存寄存器，则在这个阶段将需要使用的保存寄存器的值保存至栈上。 主体部分：执行函数的功能。 “结语”部分：按照后进先出的顺序将栈上的值恢复到相应的保存寄存器中并恢复栈指针，然后return。 5. 举例详解 假设我们要使用RISC-V指令实现下面的c语言代码 int addTwo(int a, int b) return a + b;int main() int a = 1, b = 2; int c = addTwo(a, b); int d = a + c; return 0; main函数的实现比较简单，main函数作为addTwo函数的调用者，会假定addTwo函数会为其保存保存寄存器中的内容，因此对于函数调用后仍需要使用到的a变量，main函数可以将其保存在保存寄存器中，完成函数调用后仍然可以直接使用。 main:\t# 1.变量初始化\taddi s0, x0, 1\t# s0 = 1\taddi t0, x0, 2\t# t0 = 2\t# 2.将函数参数放进参数寄存器中\taddi a0, s0, 0\taddi a1, t0, 0\t# 3.函数调用\tjal ra, addTwo\t# 4.取得函数返回值\taddi s1, a0, 0\t# s1 = addTwo(a, b)\t# 5.继续执行剩下指令\tadd s1, s1, s0\t# s1 = s1 + s0\tjal x0, exit 对于addTwo函数，根据约定，其在执行功能前要保存其要使用的保存寄存器的值，完成功能后要恢复其值。同时要注意的是，a0寄存器既起到了传递参数的作用也起到了传递返回值的作用，八个参数寄存器中只有a0和a1寄存器可以用来保存返回值，不过C语言中只能返回一个值。（实际上实现这个函数并不一定需要使用保存寄存器，在此仅仅是为了举例） #================================================## Function: add two numbers# Arguments:# a0 (int) is the first number to be added#\ta1 (int) is the second number to be added# Returns:#\ta0 (int) is the sum of the two numbers above #================================================#addTwo:\t# 序言，压栈，保存s0中的值\taddi sp, sp, -4\tsw s0, 0(sp)\t# 主体，执行函数功能\tadd s0, a0, a1\t# 将返回值放入a0寄存器中\taddi a0, s0, 0\t# 结语，恢复保存寄存器的值并弹栈\tlw s0, 0(sp)\taddi sp, sp, 4\t# 将控制交还给调用者\tjalr x0, 0(ra) 假设我们将main函数中的int d = a + c;改为int d = b + c，由于我们的变量b是存储在临时寄存器t0中，所以被调用的函数不保证临时寄存器的值不发生改变，被调用函数是可以任意使用临时寄存器的。因此，我们要么将变量b存进另一个保存寄存器中，要么我们在调用函数前将其值保存至栈上，在调用函数后将其值恢复。下面是改变后的main函数片段（addTwo函数无需改动）： # 将临时变量压栈addi sp, sp, -4sw t0, 0(sp)# 调用函数jal ra, addTwo# 恢复临时变量值lw t0, 0(sp)addi sp, sp, 4 当然，在这个例子中即使不保存t0的值，最后也能得到正确的结果，但是这样的情况是没有保证的，被调用者即使使用了临时寄存器也不用负责恢复其值。为了100%得到正确的结果，在调用函数前一定要保存之后要使用的临时寄存器，这样就无需担心被调用者是否使用了临时寄存器。 6. 进阶：递归 更复杂的情况就是递归调用，其难点在于ra寄存器的管理，由于ra寄存器不属于保存寄存器，因此每次发起递归调用时都需要保存ra寄存器，并且在调用完毕后恢复ra寄存器的值以返回到上一级调用。这里以计算阶乘为例。 int fact(int n) if (n == 1) return 1; // 为了让汇编代码步骤分明，故这里的代码写得啰嗦一些 int x = fact(n - 1); int x = n * x; return x; 计算阶乘的递归函数是一个有去有回的递归过程，当满足递归结束条件时需要从最后被调用的fact函数层层返回，每层fact函数的返回地址都是不同的，但是我们却只有一个ra寄存器，因此在我们调用下一层函数时，我们需要先保存ra寄存器中的值（ra寄存器不是保存寄存器，需要由调用者自行保存）。 #================================================## Function: compute n factorial# Arguments:# a0 (int): n# Returns:#\ta0 (int): factorial(n) #================================================#factorial:\t# if a0 == 1 return addi t0, x0, 1 beq a0, t0, return addi sp, sp, -8 sw a0, 0(sp) # save n sw ra, 4(sp) # save ra addi a0, a0, -1 # a0 = n - 1 jal ra factorial # fact(n-1) add t0, x0, a0 # t0 = fact(n-1) lw a0, 0(sp)\t# a0 = n; lw ra, 4(sp) addi sp, sp, 8 mul a0, a0, t0 # a0 = n * fact(n-1) jalr x0, 0(ra)\t# return n * fact(n-1)return: jalr x0, 0(ra)","tags":["计算机组成原理","assembly language"],"categories":["计算机基础"]},{"title":"课题分离，生活的一剂猛药","path":"/2022/10/23/课题分离，生活的一剂猛药/","content":"“关你屁事、关我屁事” pro max版 个人与外界的切割 课题分离，是个体心理学家阿德勒提出的概念。简单来说，一件事情最后造成的结果由谁承担，这件事情就是谁的课题，我们只需要管理好自己的课题，不应受到他人的干涉。 是否要好好学习、想上什么大学、要选择什么样的职业…都是关于自己想要成为一个什么样的人，这些事情通通都是我们自己的课题，不仅别人不应干涉，我们自己也不能受到他人的影响。 显然，想要实现课题分离是一件很困难的事情。在我们的生活环境中，从小到大我们基本上都被社会期望与他人期望裹挟。社会舆论具有很强的导向型，通过教育、社会宣传等方式来塑造一个社会价值最大化的人，这不是给被教育的人提供选择、告诉被教育者你们可以成为各种各样的人，而是要被教育者成为某一类人，你要如此如此、你该这样那样。他人的期望往往来自于和我们亲近的人，你的父母希望你孝顺希望你有为，你的老师希望你只有学习，你的恋人希望你给TA提供情绪价值或者物质价值…这些都是隐形的陷阱，亲近的人的期望未必是为了我们好，有时这些期望是以亲密关系为筹码的要挟，也许只是希望满足自己的想法或者控制欲望，以自我为中心的人希望世界能够如他的意。但是每个人在想问题的时候当然会首先从自我出发，毕竟每个人是其可观测世界的中心，换位思考是一种稀缺能力。 走别人铺好的路是一件诱惑性很大的事情。我们都会害怕不确定性，害怕未知的前路有隐藏的困难，害怕自己做的选择没有给自己带来预期的收获，如果有人指向一条老路告诉我们，“走吧，一直走下去就能获得他人、社会的认可，大家都是走这里的”，我们当然会动心，走在熙熙攘攘的大部队中也能让我们安心。但是，给建议的人根本不了解你的情况，他们将你简单化成一个白板、一个Person类，而不是一个有独特性格、自己的行事方式、自己的喜好的具体人，他们认为自己写了一个Person类模板，使用时直接将参数往里套就行了，编程可以如此，可惜人生却不行。 template typename T1, typename T2class Personpublic: void things_you_should_do() ... //他人不可见或不够了解的private:\tT1 character; T2 hobby; 看诸如“大学生必做十件事”、“五个方法让你的学习突飞猛进”这样的文章或者视频时，我们也会想象自己做到这些事情后能像博主一样在某些领域取得“成功”，然而大部分情况是我们看过之后根本不会去做或者完全实施不下去，我不否认其中有很多有用的信息，但无论如何选择权都应该握在我们自己手里。别人不能要求你一定要读研、一定要名列前茅、一定不能玩游戏…他人期望常常是自私、不考虑你特异性的不合理想法。且他人期待常常是一种贪心算法，他们通常要你去做当前处境下收益最大的事情，我们知道这并不能获得全局最优解；最了解自己的是我们自己，利用这些了解作为启发式信息，握紧选择权自己评判当前处境后进行启发式搜索看起来更明智一些。 可恶的他人，可怜的自己 我们不是为了满足他人的期待而活，他人也不是为了满足我们的期待而活。 ​\t我们要将自己的课题牢牢地把握在自己手里不受他人干涉，同时也要放开他人的课题，不去干涉他人。 ​\t过去的我自卑又敏感，总是处处感受到他人的“恶意”，别人看起来态度冷淡，就觉得人家很拽；别人犯了点什么无心之过，就觉得别人对自己有什么意见；别人做得比我好，就觉得他在和我较劲。然而当我和他们深入接触，他们完全不是那样的人，看起来很拽只是从小以来的仪态习惯导致，实际上对人十分和善；给我造成什么麻烦只是真的不小心；其实别人只是在默默努力，从来没有和我竞争的意思，而且很乐意成为我的伙伴；如果我一直以第一印象刻板地给他人定性，又会错过多少朋友。 究其原因，是自己内心在偷偷评价他人的课题。别人怎么待人、别人说了什么话、别人要做什么事，这些都是别人的课题，而在我的内心深处却有一套希望别人怎么对待我的想法，并且将这些想法架在了别人的课题之上。内心里希望整个世界的人都对自己慈眉善目，要是有一点不符合，不知不觉就把别人当作“敌人”，认为人人都会随时愚弄、嘲讽甚至攻击脆弱的自己，同时就可以在心中无声宣泄对他人的愤怒。对外直接宣泄愤怒对一个自卑的人来说是困难的事情，而把别人当作假想敌来在心中宣泄愤怒和厌恶则会获得一种畸形的“做自己长久以来不能做的事情”的快感。不仅是宣泄愤怒，同时也是激发自己的优越感，心里想着，“我就不会这样”、“我对他人多和善”，以自我为中心的人想要求别人都如自己的意，还从各种事物中寻找自己比别人好的可能。 放下他人的课题是对自己也是对别人的解脱，别人如何对我如何看待我，那是别人的课题，我们只要做好自己。希望能够得到所有人的认可并不代表你很关心他人，其实你只是将他人当作满足自己认可欲望的工具，希望能够通过被他人认可而获得一点点价值感，你的出发点是“自己能够被他人认可”，而不是“给他人的生活添加一点温暖”。 当然，是否要放下他人的课题这件事也是你自己的课题，说出“不要干涉别人的课题”其实已经违反了我们前面说的“自己的课题自己做主”的原则，这样就陷入了论证的矛盾，所以你可以选择放下也可以选择去干涉，只不过最后的结果需要你自己承担罢了。 找到自我 何谓“找到自我”？这是我个人的理解：能够独立评价各种事情对自己的价值的一种自立状态。 上大学前的我和这个状态只能说是相去甚远，普通学生没有选择、不需思考，考上好初中然后考好高中，最后考好大学 ，校园生活一个“卷”字足矣，什么时候是个头？你不用管，把时间全用来学习不会有错。但是一天到晚只有学习和刷题的人，心理很难不出问题，激烈的竞争、升学的压力、成绩的焦虑，持续整整一年甚至三年，只因为别人说你应该如此，而非自己认为“我希望通过这样来获得更幸福的生活”。上了大学后的我读了一些书，发现了自己的一些心理问题和十年来的教育经历以及童年生活有很大的关系。 当然我不是要否定高考和努力学习的价值，现在我是认可好好学习的重要性的，如果没有进入一个好的初中好的高中好的大学，就没办法遇到这些有想法、值得我去学习的同学们，可能就会浑浑噩噩地作为一个原子集合体度过一生，这是比一些心理问题让我更加无法接受的。 这就算是一个独立评价各种事情对自己价值的一个例子，不能独立评价的人以他人的评价作为自己的评价，他人觉得好便是好，他人觉得不好便是不好。有自我的人可以通过课题分离自己进行评价和选择。 独立是一个重点，如何评价是另一个重点。如果总是怨天尤人，抱怨童年对自己的不良影响，以过去的经历作为现在失败的原因，也不能算是有自我。诚然过去的经历一定程度上塑造了今天的我们，但是我们对过去的解释可以重新塑造未来的自己，我们给过去的生活赋予了什么样的意义，这直接决定了我们的生活。我不能说因为童年经历造成的心理问题所以导致了我今天不如别人，这是在拿过去的经历当挡箭牌，其实心里想的是“如果没有…我也能做到…”的自我安慰。自卑、敏感、好胜这些问题确实给我带来了很大的困扰，但是在尝试解决这些问题的过程中，我变得更好，如果没有这些问题，可能今天的我就没办法那么好。 这听起来非常的阿Q精神，但其实是一种从“决定论”到“目的论”的转变，“决定论”说我们的现在和将来都由过去决定，无法改变，而目的论更关注现在的“目的”。对于一个自称“社恐”的人，他可能会说“由于童年受到的什么什么创伤才导致今天的局面的”，这是原因论，而实际上，他可能是为了让自己符合自己对自己社恐的定义，而在过去的经历中找出一段符合自己目的的事情来解释，他是为了不想社交而给自己制造了“社恐”，这是目的论。 阿德勒说：“任何经历本身并不是成功或者失败的原因。我们并非因为自身经历中的刺激——所谓的心理创伤——而痛苦，事实上我们会从经历中发现符合自己目的的因素。决定我们自身的不是过去的经历，而是我们自己赋予经历的意义”。 过去的悲惨经历当然是不幸的，我们不是要否定痛苦甚至歌颂痛苦，我们只是要换一种角度去思考。把过去的苦难拿出来再折磨自己一百遍一千遍也毫无作用，反复地自证预言我们根本无法得到改变，不妨从中抽身出来，想想可能过去的不幸是被自己夸大了、或者过去虽然不幸，但是已经不能再影响今天的我了，我可以成为一个全新的人。 独立地评价各种事情对自己的价值，这就是我想要的自我。 理性的阈值 道理总是听起来很美好的，但人不是一台纯理性的执行机器，能准确无误地完成给定的指令，当生活的风雨刮来，往往什么大道理都抛到脑后了。只要我们的生物本能还在，感性在我们的大脑中永远有一席之地。 在准备上台演讲的时候，我还是会不自觉的紧张起来；在看到自己的成绩稍稍落后的时候还是会不自觉的感到失落…如果未来我的孩子（如果有的话）耽于玩乐不事学习，我能控制住自己不去干涉他的课题吗？ 德充符庄子惠子谓庄子曰：“人故无情乎？”庄子曰：“然。”惠子曰：“人而无情，何以谓之人？”庄子曰：“道与之貌，天与之形，恶得不谓之人？”惠子曰：“既谓之人，恶得无情？”庄子曰：“是非吾所谓情也。吾所谓无情者，言人之不以好恶内伤其身，常因自然而不益生也。”","tags":["读书","随笔"],"categories":["随笔"]},{"title":"有限自动机与序列识别","path":"/2022/10/02/有限自动机与序列识别/","content":"子字符串搜索是指给定一段文本，需要在其中找到想要的字符串；如果将文本换成输入流，那么问题就转换成了序列识别；而有限自动机则是一个能够在线性时间内完成以上任务的一种数学模型。 1 使用有限自动机进行子字符串搜索的基本过程 AUGGCUCCUCUGUAA 假设这是mRNA上的一串密码子，而我们想要找到这串序列中密码子CCU的位置。 1.1 定义状态 要使用有限自动机来完成这个任务，首先需要设置读取过程中的状态，为此，我们做出以下定义： 状态0: 未读取到任何符合的字符 状态1: 读取到一个符合的字符，在上面的例子中即读取到碱基C 状态2: 连续读取到两个符合的字符，即连续读取到两个碱基C 状态3: 读取到所有符合的字符，匹配成功，即检测到密码子CCU 1.2 确定状态转移 有限自动机的初始状态为0，我们一个一个地从文本中读取字符，根据读取到的字符，我们要确定有限自动机会到达的下一个状态，当达到状态3时，我们的目标才算是完成了。 举个例子，如果我们第一个读取的字符是A，这并不是我们所需要检测的第一个字符，则状态机仍然会处于状态0；但是如果我们读取的字符是C，这正是目标密码子的第一个字符，根据定义，自动机到达状态1。 如果下一个字符幸运的还是C，我们则可以继续前进到达状态2；如果不幸地检测到了G，则目前检测到的是CG，和我们的目标没法重叠，我们只能无奈地回到状态0。 当我们处在状态2时，再读取到一个U即可大功告成，但若是读取到的还是C（不考虑这种情况在生命科学中是否存在），我们既不用退回到状态0也不会退回到状态1，我们仍然会处于状态2，因为我们当前检测到的序列是CCC，如果取后两个C，我们还是符合状态2的定义，现在只需把第一个C丢掉不管即可。所以有限自动机的检测是允许重叠的，读取到一个不符合期望的字符时不一定会退回状态0，除非我们要求必须一次性读取到全部目标字符。 由此，我们可以画出完整的状态转移图，同时得到状态转移图的矩阵表示： 状态转移图状态转移矩阵 0 1 2 C 1 2 2 U 0 0 3 A 0 0 0 … 0 0 0 Z 0 0 0 按照以上步骤，给定非空有穷的状态集合、非空有穷的输入字母表、状态转移函数、初始状态、终止状态，则最终得到了有限自动机。而上面这个自动机，对于每个状态和每个输入字符，可以完全确定唯一的后继状态，则称为确定型有穷自动机(deterministic finite automata, DFA)。 我们定义一个二维数组int[R][j] dfa来表示上面的状态转移矩阵，R是输入字母表的字母个数，j是当前所处的状态，dfa[c][j]代表状态j下读取到字符c后的后继状态，该矩阵描述了状态转移函数。 对于我们要处理的问题，矩阵dfa是一个稀疏(sparse)矩阵，大部分元素是0，如果输入字母表是ASCII码表（128个字符）或者Unicode（65536个），则情况更甚。 1.3 根据状态转移表完成我们的任务 初始化自动机，状态j = 0， 读取第一个字符，为A，dfa['A'][0] = 0，仍在状态0 读取第二个字符，为U，dfa['U'][0] = 0，仍在状态0 … 读取第五个字符，为C，dfa['C'][0] = 1，到达状态1 读取第六个字符，为U，dfa['U'][1] = 0，回到状态0 读取第七个字符，为C，dfa['C'][0] = 1，到达状态1 读取第八个字符，为C，dfa['C'][1] = 2，到达状态2 读取第九个字符，为U，dfa['U'][2] = 3，到达状态3，算法终止 2 如何用代码自动生成DFA？ 给定一个要检测的序列，最容易构建的显然是DFA的主干部分，即从状态0到状态1，从状态1到状态2…从状态n-1到状态n的路径 对于剩下的匹配失败时的状态转移，我们可以这样考虑，由于在状态j读到了一个不满足期望的字符char，则目前读到的整个序列[0…j]都是不满足要求的，那么我们则剔除序列中的第一个字符，检验序列[1…j]输入自动机后会到达什么状态，该状态就是状态j读取char时会到达的状态。我们先将序列**[1…j-1]**输入状态机，检验其会达到什么状态，将这个状态称为重启状态x，再在这个状态下输入字符char，检验会到达什么状态，该状态即为状态j匹配char时会到达的状态。 比如说，我们在状态2下匹配到了C，则目前读取到的序列是CCC，我们去掉第一个字符，将第二个字符C输入状态机，最终达到状态1，则状态2重启状态为状态1，再将匹配失败的那个字符C输入状态1，最终到达状态2，则dfa['C'][2] = dfa['C'][1] = 2，即目前我们匹配成功的字符数目还是2个。 故我们可以得出dfa[匹配失败字符][j] = dfa[匹配失败字符][x], dfa[匹配成功字符][j] = j + 1。 第一个例子第二个例子 上述过程的代码如下： public void DFAConstruct(String pat) int M = pat.length(); int[][] dfa = new int[R][M]; //如果是ASCII码，则R = 128 dfa[pat.charAt(0)][0] = 1; for (int x = 0, j = 1; j M; j++) for (int c = 0; c R; c++) dfa[c][j] = dfa[c][x]; dfa[pat.charAt(j)][j] = j+1; x = dfa[pat.charAt(j)][x];","tags":["algorithm","KMP算法"],"categories":["algorithm"]},{"title":"LSTM笔记","path":"/2022/09/29/LSTM笔记/","content":"LSTM计算公式 LSTM 计算过程： 输入门：It=σ(XtWxi+Ht−1Whi+bi)遗忘门：Ft=σ(XtWxf+Ht−1Whf+bf)输出门：Ot=σ(XtWxo+Ht−1Who+bo)输入门：I_t = \\sigma(X_tW_{xi}+H_{t-1}W_{hi}+b_i) \\\\ 遗忘门：F_t = \\sigma(X_tW_{xf}+H_{t-1}W_{hf}+b_f) \\\\ 输出门：O_t = \\sigma(X_tW_{xo}+H_{t-1}W_{ho}+b_o) 输入门：It​=σ(Xt​Wxi​+Ht−1​Whi​+bi​)遗忘门：Ft​=σ(Xt​Wxf​+Ht−1​Whf​+bf​)输出门：Ot​=σ(Xt​Wxo​+Ht−1​Who​+bo​) 其中It,Ft,Ot∈Rn×hI_t,F_t,O_t\\in R^{n\\times h}It​,Ft​,Ot​∈Rn×h，n为batch_size 候选记忆细胞：C~t=tanh(XtWxc+Ht−1Whc+bc)候选记忆细胞：\\tilde{C}_t=tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c) 候选记忆细胞：C~t​=tanh(Xt​Wxc​+Ht−1​Whc​+bc​) 其中C~t∈Rn×h\\tilde{C}_t \\in R^{n \\times h}C~t​∈Rn×h 记忆细胞：Ct=Ft⨀Ct−1+It⨀C~t隐藏状态：Ht=Ot⨀tanh(Ct)记忆细胞：C_t=F_t \\bigodot C_{t-1}+I_t \\bigodot \\tilde{C}_t \\\\ 隐藏状态：H_t=O_t \\bigodot tanh(C_t) 记忆细胞：Ct​=Ft​⨀Ct−1​+It​⨀C~t​隐藏状态：Ht​=Ot​⨀tanh(Ct​) ⨀\\bigodot⨀表示按元素乘法 要点： 输入门ItI_tIt​、遗忘门FtF_tFt​、输出门OtO_tOt​、候选记忆细胞C~t\\tilde{C}_tC~t​的计算与传统RNN中隐藏状态H的计算相似 遗忘门决定上一个记忆细胞的信息是否流入该时间步的记忆细胞，输入门决定该时间步的候选记忆细胞的信息是否流入该时间步的记忆细胞 输出门控制改时间步的记忆细胞的信息是否流入该时间步的隐藏状态并被输出层利用 多层LSTM： 向量化： 输入X：(batch_size,seq_len,d_model)输入X：(batch\\_size,seq\\_len,d\\_model)输入X：(batch_size,seq_len,d_model) 隐藏状态H：(batch_size,num_direction∗num_layers,num_hidden)隐藏状态H：(batch\\_size,num\\_direction*num\\_layers,num\\_hidden)隐藏状态H：(batch_size,num_direction∗num_layers,num_hidden) 记忆细胞C：(batch_size,num_direction∗num_layers,num_hidden)记忆细胞C：(batch\\_size,num\\_direction*num\\_layers,num\\_hidden)记忆细胞C：(batch_size,num_direction∗num_layers,num_hidden) 图片来源：TensorFlow入门（五）多层 LSTM 通俗易懂版_永永夜的博客-CSDN博客_lstm层数","tags":["deep-learning","NLP"],"categories":["deep-learning"]},{"title":"Hello World","path":"/2022/09/29/hello-world/","content":"Hello world!"},{"title":"关于我","path":"/about/index.html","content":"简介游戏读书素描博主目前是一名在校研究生，本站的内容主要是博主学习过程中的一些想法或者笔记，也可能写一些其他的感悟，邮箱：liuzf042@163.com以撒的结合，游戏时长600h+，在肝全成就(看来这辈子是肝不出来了)女神异闻录3/4/5，全二周目 + 全成就上古卷轴5，游戏时长157h，已全成就荒野大镖客：救赎2，游戏时间145h动物森友会，偶尔登陆塞尔达传说：旷野之息/王国之泪刺客信条系列（艾吉奥到马拉卡）火影忍者疾风传：究极风暴4，游戏时长48h，已全成就侠客风云传系列超级马里奥：奥德赛 读的书比较杂，涉及中外小说（主要）：三体（♥）、银河帝国、百年孤独（♥）、刀锋、金庸小说…一点国学著作：这方面比较少，主要是老庄心理自助类书籍：上了大学需要寻找自我、需要修复被刷题扭曲的心态，被讨厌的勇气、活出最乐观的自己之类，最近不怎么读林林总总读了近百本书，虽然不多，但是也尽量每个月都坚持读（从学习和打游戏里挤点时间）。对世界还没有什么深刻的思考，不过在那之前还是先立己、先完善自身。大四下学期已经保研，毕设也基本完成，终于有了比较多的个人时间，正好利用起来学点技能，所以从2025.04.26开始学画素描，没有收入只能在网上自学，但是图个自娱自乐也不要求达到多高的水平。画画和编程都算有创造性的过程，在跑通一段程序或者画完一幅画时很有成就感和满足感，希望能够坚持画下去吧。"},{"title":"友链","path":"/friends/index.html","content":"安然无恙南大大佬->上交大佬"}]